[{"content":"When I heard that the founders of Docker were building an amazing team to take another spin to the whole CI/CD space I was intrigued.\nYou see, CI/CD for me was this space where I had just \u0026ldquo;settled in\u0026rdquo; and decided it was supposed to suck. I was supposed to just deal with it, adapt to whatever was built, fix whatever issue I was facing (without any kind of testing) and move on. I thought: this is just how it works, you know. We have all these tools available that some people know how to use and they solve this problem in their own way. Some build bespoke pipelines with Jenkins. Some glue everything together with Bash and Makefiles. Some build CLIs and then orchestrate them with beautifully aligned YAML. Some don\u0026rsquo;t even bother building something at all because they find it a hassle and would rather spend their time coding.\nI very clearly remember that this is exactly what was happening before Docker was introduced. We had tools for packaging applications that people knew inside and out and you had teams of folks that would dedicate big chunks of their time to figuring this thing out. Everybody was doing their own thing as well, using stuff like Packer, AMIs and a bunch of other tools mixed together. But then Docker was introduced and the game suddenly changed. Now there was a standardized way of packaging your apps that would work for almost anything you wanted to build. And what was best: it was straightforward enough that us developers could do it without thinking we were wasting our precious coding time.\n\u0026ldquo;Changing the game\u0026rdquo; like Docker did is not an obvious thing to do. Linux containers had been around for over 10 years before someone figured out how to use them effectively to solve the problem of packaging and running applications in a standardized way. I\u0026rsquo;m not yet sure if Dagger is going to be a game changer as well, but if I were sure then I would be joining too late. Which is why I decided to take this leap to join the team and be a part of building something different.\nAfter some time I\u0026rsquo;ve realized that I like to be in companies that prioritize the people and the product instead of blindly chasing challenges just for the sake of the challenge. In the case of Dagger I found that the company is filled with people that I believe are incredible. I talked to 11 employees (out of ~20 at the time) in all kinds of roles before joining and I left each meeting feeling energized and eager to get to work with them. This is not something that happens frequently to me. On the product side, what we are building is, for me personally, incredibly interesting. It is a new take on something that, as was mentioned before, I personally feel like as an industry we\u0026rsquo;ve left for granted and assumed it is just supposed to suck. We are at the stage where we are simply coping with it.\nAfter the first conversation I had with Sam Alba I realized that there are so many things that can be done in a significantly different and better way, all we need is people that have enough resources, time and commitment to see it through. We still don\u0026rsquo;t know what the end state of all this will be and that\u0026rsquo;s okay. It\u0026rsquo;s part of the fun!\nBe a part of it! A big part of Dagger is being built in the open and you can be a part of it! We have an open source project on Github and you can join our Discord server where we discuss a lot about what we are building and what we will do next.\n","permalink":"https://blog.matiaspan.dev/posts/why-i-joined-dagger/","summary":"When I heard that the founders of Docker were building an amazing team to take another spin to the whole CI/CD space I was intrigued.\nYou see, CI/CD for me was this space where I had just \u0026ldquo;settled in\u0026rdquo; and decided it was supposed to suck. I was supposed to just deal with it, adapt to whatever was built, fix whatever issue I was facing (without any kind of testing) and move on.","title":"Why I Joined Dagger"},{"content":"This is the second post in a series of blog posts that look at Dagger from different perspectives. In this post we do a deep dive on how to leverage Dagger as a developer that is tasked with implementing the CI process of a Java-Gradle service. In this CI process we need to: build, run end to end tests with external dependencies and package the service. We will leave the CD part for a future blog post that looks at Dagger from the perspective of a Platform Engineer/SRE. We are going to build this with traditional tools like docker-compose and with Dagger modules and then compare the two approaches.\nNOTE: Dagger is in very active development so by the time you read this blog post some things might have changed.\nBackground We suddenly land on a team that is the owner of a Spring-Boot service that uses gradle as its build tool and we are tasked to build the CI process for this service. The first we think about when we look at this codebase is that we should rewrite it in Go. But we are going to decide to push that thought aside for a bit and be productive ü§µ. This service is part of an E-Commerce system and is used primarily for filtering orders that are stored inside a MySQL database. It exposes one important endpoint that accepts a list of filters and returns the orders that match those filters. For example:\nGET /orders?page=1\u0026amp;status=open\u0026amp;shipping_method=43123 { \u0026#34;orders\u0026#34;: [ { \u0026#34;id\u0026#34;: 1234, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;shipping_method\u0026#34;: 43123, \u0026#34;payment_status\u0026#34;: \u0026#34;pending\u0026#34;, \u0026#34;fulfillment_status\u0026#34;: \u0026#34;unfulfilled\u0026#34; }, { \u0026#34;id\u0026#34;: 1235, \u0026#34;status\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;shipping_method\u0026#34;: 43123, \u0026#34;payment_status\u0026#34;: \u0026#34;paid\u0026#34;, \u0026#34;fulfillment_status\u0026#34;: \u0026#34;fulfilled\u0026#34; } ] } The service has not yet reached production but we know this endpoint will be very critical. It has to be consistent, meaning that if I specify status=open as a filter, it should only return orders that match that status. If we break this constraint we are going to be in big trouble. To guarantee that we never merge or deploy code that breaks this constraint we decide is best to build a few black-box end to end tests that validate the behavior of the entire system. We are going to start our service and its dependencies and execute a program that makes HTTP requests with different filters and validates that the responses match what we expect. We want developers to be able to run this tests locally but we also want to make them a hard constraint on our CI, so we need to run them there as well. For the sake of keeping this blog post focused on the CI part, I\u0026rsquo;ll share just a snippet of what this end to end tests look like. In our repository we created a tests folder and wrote a main.go file that does what we just mentioned. Here is a snippet of what this program looks like:\nvar endpoint = flag.String(\u0026#34;endpoint\u0026#34;, \u0026#34;localhost:8080\u0026#34;, \u0026#34;service endpoint\u0026#34;) func main() { flag.Parse() if err := OrdersList(*endpoint); err != nil { log.Fatalf(\u0026#34;tests failed: %s\u0026#34;, err) } } func OrdersList(endpoint string) error { tests := []struct { name string filters Filters }{ { name: \u0026#34;open\u0026#34;, filters: Filters{ Status: OrderStatusOpen, }, }, { name: \u0026#34;open paid\u0026#34;, filters: Filters{ Status: OrderStatusOpen, PaymentStatus: PaymentStatusPaid, }, }, // more test cases } errs := []error{} for _, test := range tests { name := test.name filters := test.filters // create HTTP request and send it using Go\u0026#39;s standard library // validate the response if err = validateResponse(filters, response.Orders); err != nil { errs = append(errs, testErr(name, fmt.Errorf(\u0026#34;response for filters is invalid: %v.\\n\\tResponse: %+v\\n\\tFilters: %+v\u0026#34;, err, response.Orders, filters))) continue } } if len(errs) == 0 { return nil } rerr := errors.New(\u0026#34;tests have failed\u0026#34;) for _, err := range errs { rerr = fmt.Errorf(\u0026#34;%s\\n\\t%s\u0026#34;, rerr, err.Error()) } return rerr } func testErr(name string, err error) error { return fmt.Errorf(\u0026#34;‚ùå test %s failed. Error: %w\u0026#34;, name, err) } func validateResponse(filters Filters, orders []*Order) error { for _, order := range orders { // validate that the orders respect the filters that were specified // if it is not the case then we return an error } return nil } With this program developers can start their local environment and run go run ./tests/main.go -endpoint=localhost:\u0026lt;PORT\u0026gt;.\nIn summary, we want to build a reproducible way to:\nbuild and package our service run unit tests run end to end tests that require the service and its dependencies to be running So that we can then build a CI process that integrates all of it.\nUsing docker and docker-compose Let\u0026rsquo;s start building this using the tools you are probably already familiar with: docker and docker-compose. To build and package our service we first write a Dockerfile that:\nStarts from an image with the Java version we require already installed. Copies all of our required files and folders. Performs a gradle build using the gradle wrapper. Starts a new stage from an image with Java installed. Installs critical dependencies that are often used to troubleshoot. Copies over the .jar generated in the previous stage. Defines a command for running the service. FROM amazoncorretto:21.0.1-alpine3.18 AS base WORKDIR /app COPY src src COPY gradlew gradlew COPY gradle gradle COPY build.gradle.kts build.gradle.kts COPY settings.gradle.kts settings.gradle.kts FROM base AS build RUN [\u0026#34;./gradlew\u0026#34;, \u0026#34;clean\u0026#34;, \u0026#34;build\u0026#34;, \u0026#34;--no-daemon\u0026#34;] FROM amazoncorretto:21.0.1-alpine3.18 AS runtime RUN apk update \u0026amp;\u0026amp; apk --no-cache add ca-certificates curl tcpdump procps bind-tools RUN mkdir -p /var/log/spring WORKDIR /app COPY --from=build /app/build/libs/gradle-service-0.0.1-SNAPSHOT.jar app.jar ENV APP_PROFILE=\u0026#34;default\u0026#34; ENV JAVA_OPTS=\u0026#34;\u0026#34; CMD java $JAVA_OPTS -jar app.jar --server.port=80 --spring.profiles.active=$APP_PROFILE In this Dockerfile we are leveraging Docker\u0026rsquo;s multi stage capabilities to reduce the final size of our image and only ship the things that we need.\nWith this defined developers can now build and run the service locally without having to install java, gradle and such:\ndocker build -t gradle-service . docker run -p 8080:8080 -e APP_PROFILE=local gradle-service How about running unit tests? Well, in our Dockerfile you probably saw that we first defined a stage called base were we mounted the code of our application. This was done to re-use the Dockerfile for running our unit tests. We can do this by leveraging BuildKit\u0026rsquo;s ability to target specific stages like so:\ndocker build -t gradle-service-base --target base . docker run -e APP_PROFILE=test gradle-service-base ./gradlew clean test With this approach we were able to containerize our build and test so that these processes run in the exact same way on all machines. Something I\u0026rsquo;ve seen quite frequently, specially in the Java ecosystem, is that developers usually run build and test commands in their CI workflows by re-using whatever abstraction the CI runtime provides. For example, in the case of Github workflows I\u0026rsquo;ve seen people often use the gradle action to run gradle commands directly in their CI. What usually ends up happening is that the action sets things up in a different way and then tests end up failing in the CI but not in the local environment of the developer. Leaving people confused and forced to ignore the tests in that PR because who has time to debug the CI?\nNow for the interesting part: end to end tests. We mentioned previously that our service uses MySQL to store and retrieve the orders it is requested. To be able to run these tests we need to have MySQL running and the service connected to it. Since we want this to be reproducible and run in our CI as well we will leverage docker-compose:\nversion: \u0026#39;3.9\u0026#39; services: mysql: image: mysql:8.2.0 container_name: mysql environment: - MYSQL_ROOT_PASSWORD=password - MYSQL_DATABASE=database volumes: - ./db/db.sql:/docker-entrypoint-initdb.d/db.sql ports: - 3306:3306 healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;mysqladmin\u0026#34; ,\u0026#34;ping\u0026#34;, \u0026#34;-h\u0026#34;, \u0026#34;localhost\u0026#34;] timeout: 5s retries: 10 networks: [\u0026#34;service\u0026#34;] service: build: . container_name: service environment: - \u0026#34;DB_HOST=mysql\u0026#34; - \u0026#34;DB_PORT=3306\u0026#34; - \u0026#34;APP_PROFILE=default\u0026#34; ports: - 8080:80 depends_on: mysql: condition: service_healthy links: - \u0026#34;mysql:mysql\u0026#34; networks: [\u0026#34;service\u0026#34;] networks: service: Let\u0026rsquo;s quickly go over what this docker-compose file has:\nservice: the definition of our service that will trigger a build when we run it (this is because we specified the build: . to docker-compose). We added a depends_on to MySQL with a service_healthy condition so that the service is only started once MySQL is up and accepting connections. mysql: a service that runs a MySQL 8 server with an SQL file mounted to seed some data for our tests. We specified a healthcheck that uses mysqladmin so that the service connects to it only once MySQL is ready for connections. We can now start our services locally and in any other environment by running docker-compose up.\nTime to add our tests. We also want these tests to be able to run in any host without requiring that host to have dependencies installed (other than the ability to run containers of course). For this reason we start writing a Dockerfile inside our tests folder:\nFROM golang:1.21-alpine WORKDIR /app COPY main.go main.go CMD go run main.go -endpoint $ENDPOINT Quite simple: we want Go and we want to execute our tests. We don\u0026rsquo;t care about publishing this image so no need to optimize it with multi-stage builds.\nWith this new dockerfile and our docker-compose we can now execute our tests. However, since everything is dockerized we need to run the container for our tests in the same network as the service is running so that it can access its endpoint. We also need to wait a bit until the service is running before triggering our tests:\n$ docker-compose up -d # You should wait for the service to be up and running $ cd tests \u0026amp;\u0026amp; docker build -t tests . $ docker run --network dagger-developer-perspective_service -e \u0026#34;ENDPOINT=service:80\u0026#34; --rm --name tests tests 2023/11/19 14:22:37 running validation for: open 2023/11/19 14:22:37 running validation for: open paid 2023/11/19 14:22:37 running validation for: open pending 2023/11/19 14:22:37 running validation for: open paid unpacked 2023/11/19 14:22:37 running validation for: open paid unpacked shippingMethod:table 2023/11/19 14:22:37 running validation for: open paid unpacked completedAtFrom completedAtTo 2023/11/19 14:22:37 running validation for: open paid and pending We are now ready to build our Github workflow that re-uses what we built here.\nBuilding our CI workflow I won\u0026rsquo;t go over too much detail here on the syntax for Github workflows. We want our workflow to run only on pull requests that are made against our main branch and we want it to:\nRun unit tests Build the service Run end to end tests As we saw previously, building the sevice already happens when we do a docker-compose up so we only need to take care of the other two:\nname: \u0026#39;pull request\u0026#39; on: pull_request: branches: - main jobs: build-and-test: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Build the base image run: docker build -t gradle-service-base --target base . - name: Run unit tests run: docker run gradle-service-base ./gradlew clean test - name: Start services run: docker-compose up -d - name: Check service health run: ./tests/check_health.sh - name: Run end to end tests working-directory: tests run: docker build -t tests . \u0026amp;\u0026amp; docker run --network dagger-developer-perspective_service -e \u0026#34;ENDPOINT=service:80\u0026#34; --rm --name tests tests - name: Tear down run: docker-compose down In this CI workflow we are:\nBuilding the base image that we can then use to run our tests Using the base image to run our tests Starting the service and database with docker-compose Running a script that checks the health of the service so that we run our end to end tests once the service is up Building the container image for our tests and running them in the same network as the service Tearing down our service and database You can see that we added a new step that previously was done manually: service healthcheck. This is so that the tests only run once the service is ready:\n#!/bin/bash max_attempts=5 for (( i=1; i\u0026lt;=max_attempts; i++ )) do response=$(curl -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; http://localhost:8080/health) if [ \u0026#34;$response\u0026#34; -eq 200 ]; then echo \u0026#34;Server is up and running.\u0026#34; exit 0 else echo \u0026#34;Attempt $i: Server not responding, retrying in 5 seconds...\u0026#34; sleep 5 fi done echo \u0026#34;All attempts failed, exiting with code 1.\u0026#34; exit 1 Now we can push our branch, create a pull request and see that our CI workflow is running successfully: Thoughts There were quite a few things we had to build in order to make this happen, and they were all a bit scattered:\nA Dockerfile for the service A Dockerfile for the end to end tests A docker-compose.yaml to run the services in a reproducible way A check_health.sh to check the health of the service before running our tests The .github/workflows/pr.yaml that glues everything together. Developing this wasn\u0026rsquo;t as \u0026ldquo;smooth\u0026rdquo; as I wrote it here. There were many mistakes I made in the middle that forced me to iterate a bit on how things would run locally and how they would run in the CI, for example the check_health.sh was only necessary in the CI. Having to define 3 stages in order to be able to run the tests in a dockerized way is also something that I haven\u0026rsquo;t seen people often do, they just rely on their CI runtime to also install dependencies and pray it does not break. I also had to play around with how to connect containers together so that our integraion tests can actually access the service. My developer experience here, in my opinion, wasn\u0026rsquo;t great. Probably no developer in the team will do a docker build \u0026amp;\u0026amp; docker run locally to run their tests, it requires more effort and steps than simply using their tool of choice (in this case gradle). If the CI ever breaks, it will probably go ignored until someone decides to be unhappy for a bit and fix it.\nUsing Dagger modules Can Dagger provide a better experience for building this? Let\u0026rsquo;s give it a try!\nQuick intro: Dagger allows you to define the entire lifecycle of your application (building, testing, packaging, etc) using code with one of the supported languages. Instead of having to write Dockerfiles and docker-compose files, you write everything in a Dagger module with your programming language of choice. Internally, Dagger leverages BuildKit and containers to make sure that the code you are writing runs anywhere in the exact the same way.\nSo, what do we do first? We initialize a Dagger module in our repository that will hold all the code required to build what we built before. Lets start by creating a ci folder and create a new Dagger module that will use Go:\nmkdir ci \u0026amp;\u0026amp; cd ci dagger module init --name gradle-service --sdk go Quickly inspecting the contents of the program that dagger generated we can see that it is using an object called dag to create containers and perform actions on them:\npackage main import ( \u0026#34;context\u0026#34; ) type GradleService struct {} // example usage: \u0026#34;dagger call container-echo --string-arg yo\u0026#34; func (m *GradleService) ContainerEcho(stringArg string) *Container { return dag.Container().From(\u0026#34;alpine:latest\u0026#34;).WithExec([]string{\u0026#34;echo\u0026#34;, stringArg}) } // example usage: \u0026#34;dagger call grep-dir --directory-arg . --pattern GrepDir\u0026#34; func (m *GradleService) GrepDir(ctx context.Context, directoryArg *Directory, pattern string) (string, error) { return dag.Container(). From(\u0026#34;alpine:latest\u0026#34;). WithMountedDirectory(\u0026#34;/mnt\u0026#34;, directoryArg). WithWorkdir(\u0026#34;/mnt\u0026#34;). WithExec([]string{\u0026#34;grep\u0026#34;, \u0026#34;-R\u0026#34;, pattern, \u0026#34;.\u0026#34;}). Stdout(ctx) } We can leverage dagger\u0026rsquo;s CLI to explore what the module can actually do, from the root of the repository we can run dagger -m ./ci functions and then try some of the already provided examples:\nLet\u0026rsquo;s start re-writing our module to build and test our service.Now, the interesting thing about Dagger is that modules can be published and re-used by others no matter what programming language was used to write the module, this means you can use a Go module from within a Python one. At the time of this writing modules are published at the daggerverse. If we look in the Daggerverse we can see that someone already developed a module to running gradle commands: We can leverage this module to execute the build and test commands. To use it we simply have to import it by running dagger mod use github.com/matipan/daggerverse/gradle. If you inspect the module with dagger -m github.com/matipan/daggerverse/gradle functions you\u0026rsquo;ll see, among other things, that we can define what version of the gradle image to start from. In our case we are using the JDK 21. Let\u0026rsquo;s write our Build and Test functions with this in mind:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) var GradleVersion = \u0026#34;jdk21-alpine\u0026#34; type GradleService struct { Source *Directory gradle *Gradle } func (m *GradleService) WithSource(src *Directory) *GradleService { m.Source = src return m } func (m *GradleService) Build(ctx context.Context) *Container { return m.getGradle(m.Source).Build() } func (m *GradleService) Test(ctx context.Context) *Container { return m.getGradle(m.Source).Test() } func (m *GradleService) getGradle(src *Directory) *Gradle { if m.gradle != nil { return m.gradle } m.gradle = dag.Gradle(). FromVersion(GradleVersion). WithDirectory(src). WithWrapper() return m.gradle } Let\u0026rsquo;s go over what we did here:\nWithSource: all Dagger calls run inside a sandboxed environment, so we need to explicitly provide things such as folders and env variables to the commands. getGradle: this function creates a reference to the Gradle module, specifying the version and configuring it to use the gradle wrapper. Build and Test: in this functions we simply obtain a reference to the Gradle module with our src directory mounted and run the specific operation there. We can now try our build function by running dagger -m ./ci call with-source --src \u0026quot;.\u0026quot; build:\nIn the Build function we are only executing a ./gradlew clean build, we can\u0026rsquo;t really build a final image from the output of this command. With Dockerfiles we created a multi-stage build to separate our runtime from our build. With Dagger we can do the same thing:\nfunc (m *GradleService) BuildRuntime(ctx context.Context) *Container { ctr, err := m.Build(ctx).Sync(ctx) if err != nil { log.Fatalf(\u0026#34;build failed: %s\u0026#34;, err) } return dag.Container(). From(\u0026#34;amazoncorretto:21.0.1-alpine3.18\u0026#34;). WithExec([]string{\u0026#34;apk\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;\u0026amp;\u0026amp;\u0026#34;, \u0026#34;apk\u0026#34;, \u0026#34;--no-cache\u0026#34;, \u0026#34;add\u0026#34;, \u0026#34;ca-certificates\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;tcpdump\u0026#34;, \u0026#34;procps\u0026#34;, \u0026#34;bind-tools\u0026#34;}). WithWorkdir(\u0026#34;/app\u0026#34;). WithFile(\u0026#34;app.jar\u0026#34;, ctr.File(\u0026#34;build/libs/gradle-service-0.0.1-SNAPSHOT.jar\u0026#34;)). WithEntrypoint([]string{\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java $JAVA_OPTS -jar app.jar --server.port=80 --spring.profiles.active=default\u0026#34;}) } You can see that from BuildRuntime we are calling the Build function defined previously. We then generate a new container that starts from amazoncorretto:21.0.1-alpine3.18 and mounts the application jar found in the container returned by build.\nSo far we\u0026rsquo;ve only done Build, BuildRuntime and Test. But we also want to run end to end tests and have everything integrated into Dagger. To do this we need a way to run the service and a MySQL server. Well, we are in luck. Dagger has native support for Services. With Dagger, we can define a container as a service and then use the dagger up command to start the service and its dependencies. All we have to do is write functions that return the *Service type:\nfunc (m *GradleService) Service(ctx context.Context, sqlInitDB *File) *Service { runtime := m.BuildRuntime(ctx) return runtime. WithEnvVariable(\u0026#34;DB_HOST\u0026#34;, \u0026#34;mysql\u0026#34;). WithEnvVariable(\u0026#34;DB_PORT\u0026#34;, \u0026#34;3306\u0026#34;). WithServiceBinding(\u0026#34;mysql\u0026#34;, m.Mysql(ctx, sqlInitDB)). WithExposedPort(80). WithExec([]string{\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java $JAVA_OPTS -jar app.jar --server.port=80 --spring.profiles.active=default\u0026#34;}). AsService() } func (m *GradleService) Mysql(ctx context.Context, sqlInitDB *File) *Service { return dag.Container(). From(\u0026#34;mysql:8.2.0\u0026#34;). WithEnvVariable(\u0026#34;MYSQL_ROOT_PASSWORD\u0026#34;, \u0026#34;password\u0026#34;). WithEnvVariable(\u0026#34;MYSQL_DATABASE\u0026#34;, \u0026#34;database\u0026#34;). WithFile(\u0026#34;/docker-entrypoint-initdb.d/db.sql\u0026#34;, sqlInitDB). WithExposedPort(3306). AsService() } Lets break this code down:\nIn the Service function we first call the BuildRuntime to generate a container that has our jar ready to be executed. We then create a new container that defines a mysql ServiceBinding to whatever Service the Mysql function returns. The Mysql function returns MySQL 8 container with our seed file ready to populate the DB when it starts. Now we can use the up command to start the service and its dependencies locally dagger -m ./ci up with-source --src \u0026quot;.\u0026quot; service --sql-init-db \u0026quot;db/db.sql\u0026quot;:\nDagger makes sure that the services are healthy before they connect to each other. All this runs in containers isolated in their own environments. This means that we can run this same command in other machines and expect it will work correctly.\nTo run the end to end tests using Dagger we can choose one of two approaches: i) containerize the execution of the tests and define it as a service connected to the others; ii) take advantage that tests are written in Go and simply call out to them as a library. Implementing the first one is quite straightfoward, we simply define a new function that uses the Go module from the Daggerverse and creates a container that is connected to the service:\nfunc (m *GradleService) IntegrationTests(ctx context.Context, tests *Directory, sqlInitDB *File) (string, error) { return dag.Go(). FromVersion(\u0026#34;1.21-alpine\u0026#34;). WithSource(tests). Container(). WithServiceBinding(\u0026#34;service\u0026#34;, m.Service(ctx, sqlInitDB)). WithExec([]string{\u0026#34;go\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;main.go\u0026#34;, \u0026#34;--endpoint\u0026#34;, \u0026#34;service:80\u0026#34;}). Stdout(ctx) } In this function we expect to receive the directory that holds the tests, the database file used to seed the db and we return the output that the command generates. We can now use dagger\u0026rsquo;s CLI to call this function and run the tests, all containerized and defined within a single codebase:\nThe second approach is interesting to explore. Since our tests are written in Go, we could technically just import them as a library and execute that Go code from within the execution context of our module. Since that is also containerized, it is still re-usable. To be able to do this we would have to move the code of our tests inside the ci folder, since it is a go module, import it from our module and call the function that runs the tests:\nimport \u0026#34;main/test/orderlist\u0026#34; ... func (m *GradleService) IntegrationTests(ctx context.Context, sqlInitDB *File) error { svc, err := dag.Host().Tunnel(m.Service(ctx, sqlInitDB)).Start(ctx) if err != nil { return err } defer svc.Stop(ctx) endpoint, err := svc.Endpoint(ctx) if err != nil { return err } return orderlist.Test(endpoint) } In this code we are importing the tests as a library, starting the service and the MySQL database, opening up a tunnel from the container of the service to the container our module is running on and then use the created endpoint to run the tests (the warning messages are from the Stop function, we can ignore them):\nI wanted to show both approaches since you might prefer to write your end to end tests using a different language to the one used by your Dagger module. If this is the case, you can use the first approach of containerizing and executing a command there.\nBuilding our CI workflow Let\u0026rsquo;s put all of this together in our CI workflow. This is where Dagger shines in my opinion. We don\u0026rsquo;t have to do anything special to make sure this works in our CI environment. We just need to install the dagger CLI and run the exact same command developers run locally:\nname: \u0026#39;pull request\u0026#39; on: pull_request: branches: - master - main - develop jobs: build-and-test: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Install Dagger CLI run: cd /usr/local \u0026amp;\u0026amp; { curl -L https://dl.dagger.io/dagger/install.sh | sh; cd -; } - name: Test run: dagger -m ./ci call with-source --src \u0026#34;.\u0026#34; test - name: End to end tests run: dagger -m ./ci call with-source --src \u0026#34;.\u0026#34; integration-tests --tests \u0026#34;./tests\u0026#34; --sql-init-db \u0026#34;./db/db.sql\u0026#34; And we are done! Less YAML and no surprises. Here is the pipeline working as we expected:\nThoughts Through the use of code and Dagger\u0026rsquo;s capabilities for running containerized services and commands we were able to build our entire CI workflow in a programmatic way inside a single Go file. Here is an overview of what we needed to write in order to make this work:\nNotice how we wrote all the important bits directly in code, no need for YAML and config files like Dockerfiles. Even though there are a few rough edges in Dagger (mainly due to how new it is) I personally believe this is a much better experience than what we had before. We wrote the entire definition of our CI using code and we were able to validate every single part of it locally. If we compare to how we could run the tests before, we now have an DX that developers can definitely use: dagger -m ./ci call with-source --src \u0026quot;.\u0026quot; test.\nConclusion In this blog post we compared the experience of using traditional tools like docker-compose with Dagger for building a CI workflow that builds and tests a Java-Gradle service. We saw how with docker and docker-compose we had to write a lot of configuration files and YAML to glue things together in a way that was not easily reproducible. Meaning that developers would run things locally in a way that would be different to how the CI would work. However, with Dagger we saw that it was no longer necessary to write this config and YAMl files. Instead, we built everything using Go code, we leveraged Dagger\u0026rsquo;s native support for running services and we glued everything together in a programmatic but still declarative way. We were then able to run this code in the CI in the exact same way that we run it locally.\nI personally believe there is still a bit of a learning curve to Dagger. It requires a mindset shift in order to stop thinking that this processes should be written and defined with configuration \u0026ldquo;languages\u0026rdquo; such as YAML and dockerfile. However, I think Dagger provides a lot of power with it\u0026rsquo;s code-first approach and its ecosystem that will only get bigger and better. It\u0026rsquo;s interesting how easily it integrates with existing CI systems and gives a developer experience that allows us to treat pipelines as software and write them using our editors and IDEs.\n","permalink":"https://blog.matiaspan.dev/posts/exploring-dagger-streamlining-ci-cd-pipelines-with-code/","summary":"This is the second post in a series of blog posts that look at Dagger from different perspectives. In this post we do a deep dive on how to leverage Dagger as a developer that is tasked with implementing the CI process of a Java-Gradle service. In this CI process we need to: build, run end to end tests with external dependencies and package the service. We will leave the CD part for a future blog post that looks at Dagger from the perspective of a Platform Engineer/SRE.","title":"Exploring Dagger: Streamlining CI/CD Pipelines with Code ‚Äì A Developer's Guide"},{"content":"Dagger is a new tool that promises to fix the yaml and custom scripts mess that CI/CD currently is by building pipelines as code with one of the supported SDKs. I\u0026rsquo;m in the process of learning this tool, understanding where it may fall short and where it shines and I decided that sharing some of the exploration I do and the learnings it leaves me with would be useful. This is the first post in a series of blog posts that look at Dagger from different perspectives. In this post I do a deep dive on how to build a pipeline for an IaC repository. I explore building this using \u0026ldquo;native\u0026rdquo; Github actions, the Dagger client and the newer approach of Dagger modules.\nNOTE: Dagger is in very active development so by the time you read this blog post some things might have changed.\nBackground We have a repository that holds all of our infrastructure in a declarative way using Pulumi\u0026rsquo;s Go SDK. Every time we want to provision, change or delete some infrastructure we need to:\nCreate a pull request with the necessary changes Wait for CI to show what changes will be applied by posting a comment to the PR Merge it Wait for CI on the main branch to apply those changes to the given environment This process will utilize two operations pulumi provides:\npulumi preview: show a diff of the changes that would be applied. pulumi up: apply the requested changes. We are going to build this workflow using Github actions first to understand how the current \u0026ldquo;status quo\u0026rdquo; works when it comes to building these types of pipelines. Then, we will build this same solution using Dagger. There is currently two ways of building this with Dagger and we will explore both: i) using the \u0026ldquo;traditional\u0026rdquo; dagger client; and ii) using the newer concept of Dagger modules.\nUsing Github actions Intro When you build pipelines with Github actions you have to write a YAML file on .github/workflows that contains the definition of your workflow, where each step can:\nRun specific commands such as curl. Refer to existing Actions that are a part of Github\u0026rsquo;s marketplace. These actions usually either install dependencies on the runner or perform specific actions. Refer to a custom action defined within the repository (or an external repo if you are paying for Github). For example, if we want to run go build on a Go project we would re-use the actions/setup-go function that installs Go in the runner where our command executes and simply call go build:\nname: Build on: - pull_request jobs: build: name: Build runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-go@v3 with: go-version: \u0026#39;stable\u0026#39; - name: Go build run: go build In this case we used the already provided setup-go action. If our tool would not have already been supported, we would have had to build the action ourselve. Refer to this blog post to learn how you would build one.\nBuilding our CI Our CI process has two separate workflows for:\nPreviewing changes: used when a PR is created against main to execute pulumi preview and show the diff as a comment on the pull request. Applying changes: used when a commit is pushed to main to run pulumi up and apply the desired changes. In both workflows we will to run a specific pulumi command. To do this we could create a workflow that has a step that installs Pulumi\u0026rsquo;s CLI and another that calls the command. However, if we browse the Github marketplace we will see that the team behind Pulumi has already built a custom action that allows us to do just that. Our Workflow that reuses this action looks like this:\nname: Preview infrastructure changes on Pull Requests on: - pull_request jobs: preview: name: Preview runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-go@v3 with: go-version: \u0026#39;stable\u0026#39; - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-region: ${{ secrets.AWS_REGION }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} - run: go mod download - uses: pulumi/actions@v3 with: command: preview stack-name: org-name/stack-name comment-on-pr: true github-token: ${{ secrets.GITHUB_TOKEN }} env: PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }} In this workflow we are: i) setting up Go since pulumi requires Go to be installed; ii) setting up AWS\u0026rsquo;s credentials required by Pulumi; iii) use Pulumi\u0026rsquo;s action to run the command. As you can see, using Pulumi\u0026rsquo;s action allowed us to solve two problems at once:\nRun the pulumi preview command by setting command: preview and post a PR Comment with the diff the preview generated. The up workflow would be very similar to this one with the small change of removing comment-on-pr.\nThoughts Given the simplicity of this workflow I don\u0026rsquo;t have much to complain about. The two things I personally did not like where:\nDeveloping it: seems like a shallow complaint, but I personally do not like the \u0026ldquo;development experience\u0026rdquo; that YAML gives you. Understanding which parameters the action accepted required me to look at a README on the action\u0026rsquo;s repo and one trial and error. Testing: it is not easy to validate this workflows locally. You could use a custom tool such as act but it does not offer a good DX and it is often quite heavy. It is true that as the CI process evolves and gets more complicated this two things become more problematic. But for this simple use case it was not too bad.\nUsing the Dagger client Intro To build a CI process using Dagger\u0026rsquo;s client you first have to choose from one of the available SDKs. We\u0026rsquo;ll use Go since our Pulumi infra is using it already. The first thing we\u0026rsquo;ll do is create a folder called ci and write a little main.go file that uses Dagger\u0026rsquo;s SDK to communicate with the Dagger engine to print a hello world:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;dagger.io/dagger\u0026#34; ) func main() { ctx := context.Background() client, err := dagger.Connect(ctx) if err != nil { log.Fatal(err) } client.Container(). From(\u0026#34;alpine:latest\u0026#34;). WithExec([]string{\u0026#34;echo\u0026#34;, \u0026#34;hello world\u0026#34;}). Stdout(ctx) } We can run this with go run or with Dagger\u0026rsquo;s CLI that shows us a detailed output of what is happening:\nInternally the Dagger engine is leveraging BuildKit to perform all the operations we ask for. This means that this code we wrote will on any machine and produce the same result. You could use this SDK to replace Dockerfiles entirely but for now we\u0026rsquo;ll focus on our IaC pipeline.\nBuilding our CI Let\u0026rsquo;s start by writing the Dagger program. Now, given that we need to support two operations (and both need the same flag) we need this program to manage arguments and execute different code based on those. Let\u0026rsquo;s rewrite our main.go to support that:\n// ... var stack = flag.Parse(\u0026#34;stack\u0026#34;, \u0026#34;prod\u0026#34;, \u0026#34;pulumi stack to use\u0026#34;) func main() { flag.Parse() // ... if len(os.Args) == 0 { log.Fatal(\u0026#34;specify an operation. Possible values: preview,up\u0026#34;) } switch os.Args[1] { case \u0026#34;preview\u0026#34;: // Run `pulumi preview --stack \u0026lt;stack\u0026gt;` case \u0026#34;up\u0026#34;: // Run `pulumi up --stack \u0026lt;stack\u0026gt;` } } As we saw previously, all we need to do is execute pulumi preview and post the result of it to Github. Since Dagger is containers all the way we can re-use existing images that have the dependencies we need. In this case all we need is pulumi, so we\u0026rsquo;ll use the official image to build a Container that has the necessary go dependencies, code and secrets to run commands:\nfunc baseContainer(client *dagger.Client) *dagger.Container { pulumiToken := client.SetSecret(\u0026#34;pulumi_token\u0026#34;, os.Getenv(\u0026#34;PULUMI_ACCESS_TOKEN\u0026#34;)) awsAccessKey := client.SetSecret(\u0026#34;aws_access_key\u0026#34;, os.Getenv(\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;)) awsSecretKey := client.SetSecret(\u0026#34;aws_secret_key\u0026#34;, os.Getenv(\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;)) return client. Container(). From(\u0026#34;pulumi/pulumi:latest\u0026#34;). WithSecretVariable(\u0026#34;PULUMI_ACCESS_TOKEN\u0026#34;, pulumiToken). WithSecretVariable(\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;, awsAccessKey). WithSecretVariable(\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;, awsSecretKey). WithMountedDirectory(\u0026#34;/infra\u0026#34;, client.Host().Directory(\u0026#34;.\u0026#34;)). WithWorkdir(\u0026#34;/infra\u0026#34;). WithEntrypoint([]string{\u0026#34;/bin/bash\u0026#34;}). WithExec([]string{\u0026#34;-c\u0026#34;, \u0026#34;go mod tidy\u0026#34;}) } In this baseContainer function we are building and returning a container that:\nStarts from pulumi\u0026rsquo;s oficial image pulumi/pulumi:latest Has the credentials necessary to communicate with Pulumi and AWS obtained from the environment but stored in a secure way using Dagger secrets. Has the infrastructure code mounted in the working directory /infra and has all the go dependencies that our pulumi code requires. This container can now be used to execute pulumi commands:\n// ... func main() { // ... switch os.Args[1] { case \u0026#34;preview\u0026#34;: out, err := baseContainer(client). WithExec([]string{\u0026#34;-c\u0026#34;, fmt.Sprintf(\u0026#34;pulumi preview --stack %s --non-interactive --diff\u0026#34;, *stack)}). Stdout(ctx) if err != nil { log.Fatal(err) } fmt.Println(out) case \u0026#34;up\u0026#34;: // Run `pulumi up` } } Running this locally using Dagger\u0026rsquo;s CLI shows a lot of output, so I\u0026rsquo;ll show here only the relevant pulumi bits:\n‚îÉ @ Previewing update..... ‚îÉ @ Previewing update..... ‚îÉ pulumi:pulumi:Stack: (same) ‚îÉ [urn=urn:pulumi:prod::networking::pulumi:pulumi:Stack::networking-prod] ‚îÉ ~ aws:ecs/service:Service: (update) ‚îÉ [id=arn:aws:ecs:us-west-2:831445021348:service/services-0187c44/ninjas] ‚îÉ [urn=urn:pulumi:prod::networking::awsx:ecs:FargateService$aws:ecs/service:Service::ninjas] ‚îÉ [provider=urn:pulumi:prod::networking::pulumi:providers:aws::default_5_35_0::321a9293-c7af-42be-a9fd-061afc82e317] ‚îÉ ~ networkConfiguration: { ‚îÉ ~ securityGroups: [ ‚îÉ ~ [0]: \u0026#34;sg-0b2ee5e5403c1ca86\u0026#34; =\u0026gt; \u0026#34;sg-0cf0cbf24b1e39535\u0026#34; ‚îÉ ] ‚îÉ } ‚îÉ - aws:ec2/securityGroup:SecurityGroup: (delete) ‚îÉ [id=sg-0b2ee5e5403c1ca86] ‚îÉ [urn=urn:pulumi:prod::networking::aws:ec2/securityGroup:SecurityGroup::svc-sg] ‚îÉ description : \u0026#34;Allow TCP traffic on port 8080 from the VPC\u0026#34; ‚îÉ egress : [ ‚îÉ [0]: { ‚îÉ cidrBlocks: [ ‚îÉ [0]: \u0026#34;0.0.0.0/0\u0026#34; ‚îÉ ] ‚îÉ fromPort : 0 ‚îÉ protocol : \u0026#34;-1\u0026#34; ‚îÉ self : false ‚îÉ toPort : 0 ‚îÉ } ‚îÉ ] ‚îÉ ingress : [ ‚îÉ [0]: { ‚îÉ cidrBlocks : [ ‚îÉ [0]: \u0026#34;10.1.0.0/16\u0026#34; ‚îÉ ] ‚îÉ description: \u0026#34;allow TCP traffic on 8080 from the VPC\u0026#34; ‚îÉ fromPort : 8080 ‚îÉ protocol : \u0026#34;tcp\u0026#34; ‚îÉ self : false ‚îÉ toPort : 8080 ‚îÉ } ‚îÉ ] ‚îÉ name : \u0026#34;svc-sg-6648e89\u0026#34; ‚îÉ revokeRulesOnDelete: false ‚îÉ vpcId : \u0026#34;vpc-0ec0d8d2118fc95c5\u0026#34; ‚îÉ Resources: ‚îÉ ~ 1 to update ‚îÉ - 1 to delete ‚îÉ 2 changes. 59 unchanged You probably already saw a big difference in how CI gets built with Dagger. We first developed the code that performs the operations we want and tested it locally instead of writing a YAML and pushing it to the repo to test it. To run this in Github\u0026rsquo;s CI environment we, unfortunately, have to write some YAML (maybe Dagger cloud will fix that some day? üëÄ):\nname: preview on: pull_request: branches: - main jobs: dagger: runs-on: ubuntu-latest permissions: write-all steps: - name: Checkout uses: actions/checkout@v3 - name: Setup Go uses: actions/setup-go@v4 with: go-version: \u0026#39;\u0026gt;=1.21\u0026#39; - name: Install Dagger CLI run: cd /usr/local \u0026amp;\u0026amp; { curl -L https://dl.dagger.io/dagger/install.sh | sh; cd -; } - name: Preview infrastructure changes env: PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }} AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} AWS_REGION: \u0026#34;us-west-2\u0026#34; run: dagger run go run ./ci preview -stack=\u0026lt;your stack\u0026gt; In this pipeline you can see that we first have to setup the tools we need (Go and Dagger, although Dagger is not strictly necessary) and then we call to our go program using Dagger\u0026rsquo;s CLI and specifying that we want to perform the preview operation on a specific stack.\nThis pipeline is missing the capability of posting a comment with the diff that preview generated. Since we are writing Go code for our CI, we can extend our program to use Github\u0026rsquo;s Go SDK and post the comment with the output of the command:\n// ... switch os.Args[1] { case \u0026#34;preview\u0026#34;: out, err := baseContainer(client). WithExec([]string{\u0026#34;-c\u0026#34;, fmt.Sprintf(\u0026#34;pulumi preview --stack %s --non-interactive --diff\u0026#34;, *stack)}). Stdout(ctx) if err != nil { log.Fatal(err) } // if the token is not specified we won\u0026#39;t post anything token := os.Getenv(\u0026#34;GITHUB_TOKEN\u0026#34;) if token == \u0026#34;\u0026#34; { return } // GITHUB_REPOSITORY is in the format: `:owner/:repo` repo := strings.Split(os.Getenv(\u0026#34;GITHUB_REPOSITORY\u0026#34;), \u0026#34;/\u0026#34;) // On pull requests GITHUB_REF has the format: `refs/pull/:prNumber/merge` ref, err := strconv.Atoi(strings.Split(os.Getenv(\u0026#34;GITHUB_REF\u0026#34;), \u0026#34;/\u0026#34;)[2]) if err != nil { log.Fatal(err) } return postComment(ctx, out, token, repo[0], repo[1], ref) } func postComment(ctx context.Context, content, githubToken, owner, repo string, pr int) error { body := fmt.Sprintf(\u0026#34;```\\n%s\\n```\u0026#34;, content) client := github.NewClient(nil).WithAuthToken(githubToken) _, _, err := client.Issues.CreateComment(ctx, owner, repo, pr, \u0026amp;github.IssueComment{ Body: \u0026amp;body, }) return err } This can still be tested locally. All we have to do is specify the GITHUB_TOKEN environment variable so that our program can communicate with Github\u0026rsquo;s API.\nHere is a sample of this working at my repository:\nImplementing the up command would be very similar to the preview but without calling the postComment function.\nThoughts You can probably see how much code we had to write in order to implement the same functionality that a few lines of YAML gave us. In the case of Github actions, all we had to do was reuse Pulumi\u0026rsquo;s action and specify an option of comment-on-pr to get everything we wanted. With the Dagger SDK we had to first build the container that had the dependencies we needed and then use Github\u0026rsquo;s SDK to post the comment we wanted. If Pulumi\u0026rsquo;s action was not already provided and we had to build it ourselves then I do believe that the approach of using Dagger, where we can run it locally and \u0026ldquo;tap\u0026rdquo; into the ecosystem of the language we are using, would have been much more superior.\nWhile this example in particular is rather simple, imagine how this process get more complicated when you want to run integration tests that require multiple containers running. I\u0026rsquo;m doing some exploration on this part that I hope to post soon, but in the meantime you can refer to this YouTube video to see an example of a more complicated CI process being implemented with Dagger.\nUsing Dagger modules Intro Dagger modules is, at the time of this writing, a brand new concept the Dagger team implemented to address some of the problems we mentioned previously. The two that we will focus on here are:\nRe-usability: with the Dagger SDK we built a program that can run pulumi commands, but it is not re-usable by myself or any other person that might need to do this same thing. Declarative Interface: our Go program is very \u0026ldquo;raw\u0026rdquo; when it comes to declaring its API. If you want to understand what is the \u0026ldquo;API\u0026rdquo; of this CLI, you have to look at the code, see the switch statement and notice that we support up and preview as valid values. When it comes to re-usability the first thing we need to understand is: how can we make what we built available to the world? Similar to how the Github marketplace allows us to browse all available actions, the Daggerverse (still in very early stages) allows us to browse all modules that were developed and published by someone. For example, here is a module that someone (üëÄ) built to run gradle tasks:\nTo understand the API that this module has we can use Dagger\u0026rsquo;s CLI and run dagger -m github.com/matipan/daggerverse/gradle functions:\nThe most interesting bit for me is that you can call other Dagger modules from your own no matter the programming language they are using. For example, the gradle module was built using Go but we can call the gradle.Build function from another Dagger module built with Python:\nimport dagger from dagger.mod import function @function def build() -\u0026gt; dagger.Container: return dagger.gradle().build() When we call this function on our module and we can see that it indeeds uses the Gradle module to perform a build (it failed because, fortunately, we are not writing Java):\nBuilding our CI Since Dagger modules can be reused and we can find all modules in the Daggerverse lets see if someone already built a Pulumi module:\nNo luck üòï. Well, at the time of this writing the Daggerverse was announced less than a week ago so it makes sense that it is not as \u0026ldquo;crowded\u0026rdquo; as the Github marketplace. But this is great for us. We get to build a pulumi module ourselves, publish it and contribute to this brand new environment that has a lot of promise. Then we\u0026rsquo;ll build our CI process using it.\nBuilding the Pulumi module We want to build a module that has an interface similar to Pulumi\u0026rsquo;s Github action where we can run any of the supported pulumi commands. But since we are going to be building it with code we can make it a bit more expressive. We start by initializing our module and specifying that we want to use Go:\ndagger mod init --name pulumi --sdk go This command will generate Go code that we will use to interface with Dagger\u0026rsquo;s SDK as well as a dagger.json file that contains the definition of our module and its dependencies (in this case we won\u0026rsquo;t have any). The sample main.go file that it generated gives us some idea on how to start writing our module:\npackage main import ( \u0026#34;context\u0026#34; ) type Pulumi struct {} // example usage: \u0026#34;dagger call container-echo --string-arg yo\u0026#34; func (m *Pulumi) ContainerEcho(stringArg string) *Container { return dag.Container().From(\u0026#34;alpine:latest\u0026#34;).WithExec([]string{\u0026#34;echo\u0026#34;, stringArg}) } // example usage: \u0026#34;dagger call grep-dir --directory-arg . --pattern GrepDir\u0026#34; func (m *Pulumi) GrepDir(ctx context.Context, directoryArg *Directory, pattern string) (string, error) { return dag.Container(). From(\u0026#34;alpine:latest\u0026#34;). WithMountedDirectory(\u0026#34;/mnt\u0026#34;, directoryArg). WithWorkdir(\u0026#34;/mnt\u0026#34;). WithExec([]string{\u0026#34;grep\u0026#34;, \u0026#34;-R\u0026#34;, pattern, \u0026#34;.\u0026#34;}). Stdout(ctx) } We want to start by implementing the 4 operations that the Pulumi CLI github action supports: up, refresh, destroy and preview. We want each of these operations to be its own function so that when we call dagger functions we can see each operation and users can refer to any one of them separately. Pulumi\u0026rsquo;s main role is to perform operations against our cloud provider so we need to solve the problem of specifying credentials. When we built the Github workflow we had to run the configure-aws-credentials step that left the environment ready for pulumi to communicate with AWS. We can do something similar here but \u0026ldquo;natively\u0026rdquo; within the module. For each cloud provider we want to support we will add a function called With\u0026lt;Provider\u0026gt;Credentials that sets the credentials in a secure way for that specific cloud provider using Dagger\u0026rsquo;s native support for Secrets. We\u0026rsquo;ll do the same thing for specifying pulumi\u0026rsquo;s access token. I\u0026rsquo;ll show here a portion of the code that implements the Up operation to keep it brief, you can see the entire code base of the module on Github:\ntype Pulumi struct { AwsAccessKey *Secret AwsSecretKey *Secret PulumiToken *Secret Version string } // FromVersion is an optional function that users can use to specify // the version of pulumi\u0026#39;s docker image to use as base. func (m *Pulumi) FromVersion(version string) *Pulumi { m.Version = version return m } // WithAwsCredentials sets the AWS credentials to be used by Pulumi. // Call this function if you want pulumi to point your changes to AWS. func (m *Pulumi) WithAwsCredentials(awsAccessKey, awsSecretKey *Secret) *Pulumi { m.AwsAccessKey = awsAccessKey m.AwsSecretKey = awsSecretKey return m } // WithPulumiToken sets the Pulumi token to be used by Pulumi. func (m *Pulumi) WithPulumiToken(pulumiToken *Secret) *Pulumi { m.PulumiToken = pulumiToken return m } // Up runs the `pulumi up` command for the given stack and directory. // NOTE: This command will perform changes in your cloud. func (m *Pulumi) Up(ctx context.Context, src *Directory, stack string) (string, error) { return m.commandOutput(ctx, src, fmt.Sprintf(\u0026#34;pulumi up --stack %s --yes --non-interactive\u0026#34;, stack)) } // ... // commandOutput runs the given command in the pulumi container and returns its output. func (m *Pulumi) commandOutput(ctx context.Context, src *Directory, command string) (string, error) { ct, err := m.authenticatedContainer(src) if err != nil { return \u0026#34;\u0026#34;, err } return ct. WithExec([]string{\u0026#34;-c\u0026#34;, command}). Stdout(ctx) } // authenticatedContainer returns a pulumi container with the required credentials. // Users have to set credentials for their cloud provider by using the `With\u0026lt;Provider\u0026gt;Credentials` // function. func (m *Pulumi) authenticatedContainer(src *Directory) (*Container, error) { if m.PulumiToken == nil { return nil, errors.New(\u0026#34;pulumi token is required. Use `with-pulumi-token` to set it\u0026#34;) } ct := container(src, m.PulumiToken, m.Version) switch { case m.AwsAccessKey != nil \u0026amp;\u0026amp; m.AwsSecretKey != nil: ct = ct.WithSecretVariable(\u0026#34;AWS_ACCESS_KEY_ID\u0026#34;, m.AwsAccessKey). WithSecretVariable(\u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34;, m.AwsSecretKey) default: return nil, errors.New(\u0026#34;no cloud provider credentails was provided\u0026#34;) } return ct, nil } // container obtains a base container with pulumi\u0026#39;s CLI installed. func container(src *Directory, pulumiToken *Secret, version string) *Container { if version == \u0026#34;\u0026#34; { version = \u0026#34;latest\u0026#34; } return dag. Container(). From(fmt.Sprintf(\u0026#34;pulumi/pulumi:%s\u0026#34;, version)). WithSecretVariable(\u0026#34;PULUMI_ACCESS_TOKEN\u0026#34;, pulumiToken). WithMountedDirectory(\u0026#34;/infra\u0026#34;, src). WithWorkdir(\u0026#34;/infra\u0026#34;). WithEntrypoint([]string{\u0026#34;/bin/bash\u0026#34;}). WithExec([]string{\u0026#34;-c\u0026#34;, \u0026#34;go mod tidy\u0026#34;}) } Since we are writing Go, any variable or function that starts with an Uppercase letter will be part of the interface of this module and developers will see it when calling dagger functions. A few things worth pointing out of this code:\nThere are 2 public functions (WithAwsCredentials and WithPulumiToken) that set the credentials for what we want to do and need to be called before our operation. See how they return a reference to the module itself, in Dagger-lang this means that in order for something to happen they require a subcommand (like up). If you want support for your cloud provider you are welcome to make a PR üöÄ. The Up function returns a string and an error, indicating the output of the pulumi command and an error if there was any. The authenticatedContainer function is where the magic happens. This function is private and is used to create a container that has the required credentials and dependencies. Users of the module can point to a specific version of pulumi using FromVersion and if they don\u0026rsquo;t we default to latest. We can test this module locally with my IaC repository. For example the Preview operation:\ndagger -m github.com/matipan/daggerverse/pulumi call with-aws-credentials --aws-access-key \u0026#34;\u0026lt;AWS_ACCESS_KEY_ID\u0026gt;\u0026#34; --aws-secret-key \u0026#34;\u0026lt;AWS_SECRET_ACCESS_KEY\u0026gt;\u0026#34; with-pulumi-token --pulumi-token \u0026#34;\u0026lt;PULUMI_ACCESS_TOKEN\u0026gt;\u0026#34; preview --src \u0026#34;.\u0026#34; --stack \u0026#34;\u0026lt;PULUMI_STACK\u0026gt;\u0026#34; And the output:\nYou can see all the operations this module supports by running dagger -m github.com/matipan/daggerverse/pulumi functions or browsing the homepage of the module in the Daggerverse.\nBuilding our CI re-using modules When now can see in the Daggerverse that there is a pulumi module we can leverage:\nThis module takes care of the IaC part but for the case of pull requests we wanted to post a comment showing the diff that was generated. Luckily, someone (thanks @aweris) already built a Dagger module that allows developers to run any command of the gh CLI. This means we could run gh pr comment \u0026lt;PR\u0026gt; --body '\u0026lt;contents\u0026gt;' from our Dagger code and pass the output that the Pulumi module gave us.\nTo get started writing our CI we will create a new \u0026ldquo;module\u0026rdquo; that has the operations our workflow requires. For simplicity we will call them preview and up as well. We\u0026rsquo;ll use Go since we kind of love it here (but remember that with Dagger it does not matter in what language a given module was built):\ndagger mod init --name iac --sdk go Now we can add the dependencies for the two modules we\u0026rsquo;ll use:\ndagger mod use github.com/matipan/daggerverse/pulumi dagger mod use github.com/aweris/daggerverse/gh And quickly build our module with the Preview function that uses these dependencies:\ntype Iac struct { AwsAccessKey *Secret AwsSecretKey *Secret PulumiToken *Secret } func (m *Iac) WithCredentials(pulumiToken, awsAccessKey, awsSecretKey *Secret) { m.PulumiToken = pulumiToken m.AwsAccessKey = awsAccessKey m.AwsSecretKey = awsSecretKey } func (m *Iac) Preview(ctx context.Context, src *Directory, stack string, githubToken *Secret, githubRef string) error { diff, err := dag.Pulumi(). WithAwsCredentials(m.AwsAccessKey, m.AwsSecretKey). WithPulumiToken(m.PulumiToken). Preview(ctx, src, stack) if err != nil { return err } // On pull requests GITHUB_REF has the format: `refs/pull/:prNumber/merge` pr, err := strconv.Atoi(strings.Split(githubRef, \u0026#34;/\u0026#34;)[2]) if err != nil { return fmt.Errorf(\u0026#34;githubRef did not have the correct format, expected: refs/pull/:prNumber/merge. Got: %s\u0026#34;, githubRef) } _, err = dag.Gh().Run(ctx, githubToken, fmt.Sprintf(\u0026#34;pr comment %d --body \u0026#39;%s\u0026#39;\u0026#34;, pr, diff)) return err } You can see that we are calling the Pulumi module Preview operation and then sending the output of it to the gh module so that it posts the comment on Github. Our Github workflow will now call our module instead:\nname: \u0026#39;preview\u0026#39; on: pull_request: branches: - main jobs: dagger: runs-on: ubuntu-latest permissions: write-all steps: - name: Checkout uses: actions/checkout@v3 - name: Setup Go uses: actions/setup-go@v4 with: go-version: \u0026#39;\u0026gt;=1.21\u0026#39; - name: Install Dagger CLI run: cd /usr/local \u0026amp;\u0026amp; { curl -L https://dl.dagger.io/dagger/install.sh | sh; cd -; } - name: Preview infrastructure changes env: PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }} AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} AWS_REGION: \u0026#34;us-west-2\u0026#34; GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: dagger -m ./ci call with-credentials --pulumi-token $PULUMI_ACCESS_TOKEN --aws-access-key $AWS_ACCESS_KEY_ID --aws-secret-key $AWS_SECRET_ACCESS_KEY preview --src \u0026#34;.\u0026#34; --stack \u0026#34;ninjastructure/prod\u0026#34; --github-token $GITHUB_TOKEN --github-ref $GITHUB_REF The dagger command that was specified in the last step can be run locally which allows us to test things before pushing them.\nThoughts As you can see at the time of this writing the Daggerverse is quite new and does not have many modules already provided. This required us to build the module to interact with Pulumi. However, we were able to find a module that provided us with the capacity to post a comment on the PR that was open. Once we had these dependencies ready to go it was just a matter of referencing them and piping things together, all with code using the programming language I preferred. With the wiring of those modules taken care of it was just a matter of using Dagger\u0026rsquo;s CLI to call our module and perform the operations we wanted. The same command we run locally to test that everything worked correctly was then added to the CI process to finish the entire workflow. If we ever were to migrate from Github actions to say, CircleCI, then it would be a matter of referencing this same command from our CircleCI workflow and that\u0026rsquo;s it.\nThe main problem I see with this approach today has to do with caching. Every single operation can potentially be cached and save us from having to recreate the entire environment every time. To achieve this with Dagger we would have to either: i) use the paid offering that provides this caching called Dagger cloud; ii) deploying a dagger engine with caching in your own infrastructure and point your CI runners to use that version of the engine. That approach is not too bad for simpler use cases, but as soon as you need to scale you\u0026rsquo;ll require multiple instances running.\nAs a side note, when we wanted to create our pipeline we had to create a new \u0026ldquo;module\u0026rdquo; that was the entrypoint for our CI. This is because the only way to use other modules today is from within another module. I found this a bit confusing. In programming languages such as Go you can create either libraries or executables. When I hear the word \u0026ldquo;module\u0026rdquo; I think of a library that requires an executable to call it and do something. Maybe it\u0026rsquo;s just a naming thing or maybe I get easily confused.\nComparison When comparing the approach of Github actions and Dagger modules I find two main differences. First of all, Github actions does not give us a process that can be easily testable and extensible. When it comes to extending we would either have to build custom actions or do some setup magic and run custom scripts on top of this yaml. Again, this is a simple process, but CI/CD often gets more complicated as time goes on. And while right now this may not seem like an obvious problem, it always ends up becoming that critical yaml file that nobody wants to touch. The second difference is more \u0026ldquo;conceptual\u0026rdquo;, but it\u0026rsquo;s important to point it out because it requires a mindset shift when it comes to building CI. In the traditional pipeline we can see a \u0026ldquo;stateful\u0026rdquo; approach where each of the steps that are being executed are doing explicit things on the host (i.e the runner) that the subsequent action then reuses:\nCheckout the code of the repository Install the latest stable version of Go Configure AWS\u0026rsquo;s credentials (technically not needed, we could use env variables directly\u0026hellip;) Download go dependencies that pulumi\u0026rsquo;s code uses. Run pulumi preview Every single one of those actions changed the underlying host in some way and left a state that the subsequent action would then use. This means that when you build a workflow you always operate directly on the host instead of \u0026ldquo;chaining\u0026rdquo; inputs/outputs like you do when building a dag, for example an Airflow dag. I don\u0026rsquo;t particularly like this approach, it is one of the main things that makes this processed hard to test and more importantly very hard to understand.\nWith dagger modules we saw a different approach. When we were calling a module we had to explicitly provide all the required resources to perform the specific operation. This is because dagger functions run in a sandboxed environment that can potentially be executed on a different host. This, in my opinion, opens up interesting possibilities for building CI pipelines using reusable APIs that are deployed once and used everywhere. There might be occasions where this stateless approach falls short but I still need to do a deep dive in those special cases. If I compare Dagger\u0026rsquo;s approach with the way the data world works I find a lot of similarities. When you build data pipelines using tools like Airflow, the worker where the operator code gets executed usually only performs HTTP calls. For example, if you build a data pipeline that performs some data transformation using spark on EMR and then ingests that data on a datasource like clickhouse, your Airflow DAG will probably look something like this:\nUse EMR Operator to execute some spark code. Airflow\u0026rsquo;s worker will call AWS\u0026rsquo;s API, ask for a given script to be executed and then wait for completion. Use an ECS task or similar to run code that reads the data from S3 and ingest it into clickhouse. Conclusion I believe Dagger offers a compelling alternative to yaml and custom scripts in CI/CD pipelines. By leveraging code, we gain expressiveness, reusability, and the ability to integrate with a wide range of tools and services. As Dagger matures, I\u0026rsquo;m excited to see how it will continue to evolve and improve CI/CD practices. However, the current state of the art of Dagger requires a big mindset shift and, while it does offer important benefits, for simpler use cases like the one explored in this blog post the learning curve seems to be a bit steep. I think that in this post we found very interesting primitives that will probably allow us to develop a better experience for all use cases.\n","permalink":"https://blog.matiaspan.dev/posts/exploring-dagger-building-a-ci-cd-pipeline-for-iac/","summary":"Dagger is a new tool that promises to fix the yaml and custom scripts mess that CI/CD currently is by building pipelines as code with one of the supported SDKs. I\u0026rsquo;m in the process of learning this tool, understanding where it may fall short and where it shines and I decided that sharing some of the exploration I do and the learnings it leaves me with would be useful. This is the first post in a series of blog posts that look at Dagger from different perspectives.","title":"Exploring Dagger: Building a CI/CD pipeline for IaC"},{"content":"Lemon Cash is a crypto startup based in Argentina that operates digital wallets, card payments and more. In this blog post I describe how we implemented a solution for provisioning, deploying and operating services that empowers developers to ship more quickly. This solution is not a fully fledged platform but it provides similar benefits by standing on top of strongly defined concepts and conventions. What we implemented only makes sense in the context in which the company was in, so I will start by explaining what that context was.\nBackground In April 2022 the company found itself at a stage many startups face, they grew really quickly without dedicated focus to infrastructure and developer experience. This means no explicit cloud networking architecture, a monolithic application deployed on ECS, a few lambdas, an RDS database and a collection of custom scripts and github actions for deploying things.\nAt that point the company decided to build a small team of Infra/SRE and Engineering Effectiveness that will tackle all things infrastructure and other big projects such as Breaking the Monolith. Before actually being able to deploy new services and break the monolith we realized that we needed to implement a \u0026ldquo;proper\u0026rdquo; cloud networking architecture along with a standardized way of provisioning, deploying and operating services. I will focus on the latter and leave the details of our networking architecture for a separate post.\nDX First When we set out to solve this problem we wanted to make sure we provided the best Developer Experience possible. We decided to go with a CLI, called lemi üçã, that acts as the interface for all thing services. In the end, we want developers to provision a service by just running:\n$ git checkout -b provision $ lemi service new --service example --cpu 512 --memory 1024 --desired-count 3 --owner infra $ git commit -a -m \u0026#34;Add service definitions and CI workflows\u0026#34; $ git push -u origin provision After that we want them to use this same CLI, lemi, to operate on their services and do things such as:\n# exec into the service\u0026#39;s container $ lemi service exec -s example # restart their service $ lemi service restart -s example # deploy an image $ lemi service deploy -s example --tag \u0026lt;image-tag\u0026gt; Concepts and conventions FTW Convention over configuration. This is something I internalized while working with Ruby on Rails a few years ago. It is incredible how much work you can not do by relying on strong conventions that do not compromise the functionality you want to provide. Coupling this with clear concepts that separate concerns allowed us to develop something very quickly while keeping a lot of simplicity.\nConcepts We started by separating Static from Dynamic Infrastructure. Static infrastructure are components such as Databases, Caches, VPN/VPCs, Lambdas, S3 buckets, Service Resources and others that require more explicit provisioning and are not changed that frequently. While Dynamic Infrastructure are things that are closer to the service itself and developer teams. They can possibly change with every deployment and should be owned and controlled by the teams themselves. This includes things like: Service versions, Observability, Routing, Access Control and pipelines. We make this distinction because we believe there different needs for each type of infra. For the case of Static Infrastructure we want to make sure that:\nIs always reproducible, All changes made to it are tracked through git and IaaC, Ownership of resources is explicit and clear, Tagging is effectively made to allow for observability and cost ownership. For the case of Dynamic Infrastructure the goal is to have it as close to the developers as possible. This means that they have full control over it and are able to work and iterate on it without the need of the infrastructure team stepping in.\nWith these concepts in mind we landed on something we are comfortable with and gives us a lot of room for improving. We have a central terraform repository that holds all of our Static Infrastructure. This central repo is owned and controlled by the infrastructure team. However, through the use of terraform modules provided by us, developer teams can very easily (and without a lot of terraform) define new infrastructure that accompanies the services they are deploying. For the Dynamic Infrastructure we provide a YAML abstraction that defines what a service is and is owned and developed by the teams themselves. This abstraction exists mainly because at the moment we are using ECS. While the platform is useful in many ways, it does not provide clear APIs that can be easily exposed to developers. We coupled this service spec with the definition of environments and their corresponding variables, secrets and parameters. The important detail is that these files live in the repository of the service itself and are owned and controlled by the developers.\nConventions The conventions that we define below allowed us to remove instead of solve many problems that don\u0026rsquo;t really limit the functionality that developers end up having and does not compromise what can be achieved. This conventions, and the problem they solve, are:\nRouting: all services are available internally through a global Application Load Balancer using Host-based routing and the name of the service as a part of our URL: \u0026lt;service-name\u0026gt;-internal.\u0026lt;environment\u0026gt;.lemon. Service Discoverability and naming collisions: the name of the service is the name of the repository. Since GitHub can\u0026rsquo;t have two repositories named the same way there won\u0026rsquo;t be any collision. Coupling this together with what was defined above for routing you can easily find which services exist in our infrastructure today. Transport: services have to expose an HTTP API on port 8080. Observability (metrics, tracing and logs): Services have to expose an endpoint /metrics on their 8080 port that holds OpenMetrics metrics. As for tracing, all services are shipped with a Datadog sidecar that provides logs and traces out of the box and automatically exports the metrics to datadog. Packaging: Docker has to be used to package the service. On Call \u0026amp; Service Ownership: the owners of the repository are the owners of the service and are in charge of operating it and being on call. Testing: we used JVM-based languages and gradle. To simplify our pipeline services have to implement three commands: ./gradlew test, ./gradlew integrationTest, ./gradlew e2eTest. To simplify the work developers have to do, they only have to implement the convention for Transport and Testing. lemi, our CLI, takes care of all the rest: Routing, Naming, Observability, Packaging and Service Ownership (by relying on GitHub permissions).\nProvisioning, Deploying and Operating a service I already threw a lot of text at you so instead of explaining things further let\u0026rsquo;s have some fun by going through a demo of what developers actually do when working with services at Lemon, let\u0026rsquo;s add some screenshots too! üì∏.\nProvisioning To simplify the demo we will assume a developer already has a part of the service developed, this means that there already is a GitHub repository that has the branches we use at Lemon: main for production and develop for staging (our testing environment).\nWe start working from main. To start the provisioning process we go back to the first part of this blog post and run:\n# first we checkout a new branch were we will add all the files generated by lemi $ git checkout -b lemi-provision # now we simply create a new service by running the following command $ lemi service new --cpu 512 --memory 1024 --owner infra --service demo-service --desired-count 1 # run git status to see what lemi actually added $ git status On branch lemi-provision Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) .github/ deploy/ nothing added to commit but untracked files present (use \u0026#34;git add\u0026#34; to track) As you can see in the last command, lemi added two new folders: deploy/ and .github/. Inspecting first deploy/ we can see that it created the service.yaml definition we previously mentioned along with two files, prod.yaml and staging.yaml, that hold the environment variables, secrets and parameters for each corresponding environment:\n$ tree deploy/ deploy/ ‚îî‚îÄ‚îÄ demo-service ‚îú‚îÄ‚îÄ environment ‚îÇ ‚îú‚îÄ‚îÄ prod.yaml ‚îÇ ‚îî‚îÄ‚îÄ staging.yaml ‚îî‚îÄ‚îÄ service.yaml 2 directories, 3 files Let\u0026rsquo;s take a look at the specification of the service by looking at service.yaml:\nname: demo-service owner: infra spec: desiredCount: 1 image: demo-service resource: cpu: 512 memory: 1024 This is a very simple spec, mainly because the requirements we have are clear and some of the conventions allow us to not provide additional config parameters. We only have to define the service\u0026rsquo;s name, owners (used for cost ownership) and the spec that holds the resources the service requires.\nWith that out the way, lets actually try to provision our service. For this all we have to do is commit, push and create a PR on GitHub. With the PR created we will see something interesting happen. There is a link, that was automatically added, pointing to a PR on our central terraform repository:\nIf we follow the link we can see what the PR is actually attempting to provision (this is the static infrastructure):\nmodule \u0026#34;demo_service_ecs\u0026#34; { source = \u0026#34;git@github.com:lemonatio/infra-ecs-module.git\u0026#34; owner = \u0026#34;Infra\u0026#34; name = \u0026#34;demo-service\u0026#34; networking = local.networking load_balancers = local.load_balancers } The interesting part here is the ECS module itself we provide. This module abstracts away all the complexity that provisioning an ECS service has and implements some of the conventions we defined previously, the most important one being routing and ingress rules that make this service accessible from within our infrastructure and from the internet. This module is closed source and I cannot share the implementation here unfortunately.\nWith this PR created, developers can add any additional Static Infrastructure that they need. For example, if their application requires an ElastiCache instance it could easily be added here using our modules as well:\nmodule \u0026#34;demo_service_ecs\u0026#34; { source = \u0026#34;git@github.com:lemonatio/infra-ecs-module.git\u0026#34; owner = \u0026#34;Infra\u0026#34; name = \u0026#34;demo-service\u0026#34; networking = local.networking load_balancers = local.load_balancers } module \u0026#34;demo_service_redis\u0026#34; { source = \u0026#34;git@github.com:lemonatio/infra-redis-module.git\u0026#34; name = \u0026#34;demo-service-redis\u0026#34; owner = \u0026#34;Infra\u0026#34; networking = local.networking service_sg_ids = toset(module.demo_service_ecs.default.network_configuration[0].security_groups) } You may be wondering how did this happen? lemi FTW! If you remember when we run the lemi service new command we showed that lemi added some github workflows within .github. One of those workflows is provision.yaml and takes care of everything we just saw. The important part of it is that the workflow does not do any magic, it only orchestrates functionality that lemi itself provides. The important part of the workflow is this right here:\n- name: Infra provisioning env: GH_AUTH_TOKEN: ${{ secrets.LEMONATIO_BOT_TOKEN }} run: | ./lemi-latest provision --prId ${{ github.event.number }} --repoName ${{ github.event.repository.name }} --currentBranchName ${{ steps.branch-name.outputs.current_branch }} --service demo-service As you can see we are using a command that lemi provides: lemi provision \u0026lt;flags\u0026gt;. The rationale for this is that we want to make sure that all of our operations that happen on our CI can be also tested and run locally. If for some reason we stopped using GitHub Workflows, developers could still simply run this command on their local machines and the entire process would continue to work.\nThis PR will be reviewed and approved by the infra team. Once it is merged the corresponding PR on the service repository will be automatically merged and the service will be considered provisioned! Now let\u0026rsquo;s move along to doing our first deployment.\nDeploying As was previously mentioned we have two environments at Lemon: production (deployed from the main branch) and staging (deployed from the develop branch). When we executed lemi service new, lemi generated the provisioning workflows along with all of our building, testing and deployment workflows. Let\u0026rsquo;s inspect staging\u0026rsquo;s deployment workflow to understand how things are deployed:\nname: Deploy to staging on: push: branches: - develop jobs: test: uses: ./.github/workflows/tests.yml build: uses: ./.github/workflows/build.yml needs: test with: image-tag: ${{ github.sha }} ecr-repo: demo-service secrets: region: ${{ secrets.AWS_STAGING_REGION }} access-key-id: ${{ secrets.AWS_STAGING_ACCESS_KEY_ID }} secret-access-key: ${{ secrets.AWS_STAGING_SECRET_ACCESS_KEY }} deploy: uses: ./.github/workflows/deploy.yml needs: build with: image-tag: ${{ github.sha }} service: demo-service environment: staging lemi-bucket: infra-lemi-staging secrets: region: ${{ secrets.AWS_STAGING_REGION }} cluster: lemon-ecs-cluster access-key-id: ${{ secrets.AWS_STAGING_ACCESS_KEY_ID }} secret-access-key: ${{ secrets.AWS_STAGING_SECRET_ACCESS_KEY }} The first important thing is reusability! You can see that each step (build and deploy) is triggering a different workflow and specifying a series of parameters that are specific for this environment. This allows us to have a single workflow that is reused across environments.\nLet\u0026rsquo;s now look into the deploy workflow since the build is rather simple (build the dockerfile and publish to ECR). The process itself is quite large, so lets only look into the relevant parts, this are the ones controlled again by lemi:\n- name: Verify service spec and provisioning id: verify-service env: SERVICE: ${{ inputs.service }} CLUSTER: ${{ secrets.cluster }} run: | ./lemi-latest verify -s=$SERVICE --cluster=$CLUSTER --check=spec,provisioned -e=$ENV - name: Compile task definition id: compile-service env: IMAGE_TAG: ${{ inputs.image-tag }} SUFFIX: ${{ inputs.suffix }} SERVICE: ${{ inputs.service }} ENV: ${{ inputs.environment }} run: | ./lemi-latest compile -t=$IMAGE_TAG -s=$SERVICE -e=$ENV cat task-definition.json - name: Deploy service with new task definition id: deploy-service env: SERVICE: ${{ inputs.service }} CLUSTER: ${{ secrets.cluster }} SUFFIX: ${{ inputs.suffix }} run: | ./lemi-latest deploy -s=$SERVICE --cluster=$CLUSTER We have three key steps defined here:\nlemi verify: this command runs a series of validations to make sure that the deployment can indeed be executed. This validation checks the service, secrets and parameters are all provisioned on the corresponding environment and validates that the YAML is correct and has all required parameters. lemi compile: with the service.yaml, the corresponding env file (i.e staging.yaml) and the ECR Image tag we compile it down into an ECS task definition lemi deploy: using the task definition created in the compilation process, we register a new revision and update the service to point to this latest version. As it was shown in the provisioning process previously, all critical operations are done by lemi and GitHub simply orchestrates the execution of it. This means that any developer can run this commands on their local machine if needed.\nWith this workflows in place all commits sent to develop will be automatically deployed to staging and those that go to main will be deployed in production:\nOperating Last but certainly not least, we want to provide a unified way of operating services at the company. This is mainly because we are not a big company and switching between teams that own different services is completely normal. So if a developer goes from team A to team B, they are already familiar with how to operate their services in our environments. This is why we built a series of commands into lemi that developers use. A few operations that developers usually do:\n# restart the service using a graceful deploy of the same image $ lemi service restart -s demo-service -c lemon-ecs-cluster # list all running tasks the service has $ lemi service list-tasks -s demo-service -c lemon-ecs-cluster [\u0026#34;arn:aws:ecs:us-east-1:\u0026lt;AWS_ACCOUNT_ID\u0026gt;:task/lemon-ecs-cluster-prod/77c0048e9f3a4e04ae6b35c89270acdd\u0026#34;,\u0026#34;arn:aws:ecs:us-east-1:\u0026lt;AWS_ACCOUNT_ID\u0026gt;:task/lemon-ecs-cluster-prod/91a67a2f88b542e4a8c30f2c6290d80e\u0026#34;,\u0026#34;arn:aws:ecs:us-east-1:\u0026lt;AWS_ACCOUNT_ID\u0026gt;:task/lemon-ecs-cluster-prod/d02725effb574e96847b4abe21740987\u0026#34;] # stop a specific task $ lemi service stop-task --task-id arn:aws:ecs:us-east-1:\u0026lt;AWS_ACCOUNT_ID\u0026gt;:task/lemon-ecs-cluster-prod/77c0048e9f3a4e04ae6b35c89270acdd # exec into the container of the service, useful for when things go wrong and we want to do closer deep dives $ lemi service exec -s demo-service -c lemon-ecs-cluster /bin/bash running /bin/bash on task arn:aws:ecs:us-east-1:\u0026lt;AWS_ACCOUNT_ID\u0026gt;:task/lemon-ecs-cluster-prod/77c0048e9f3a4e04ae6b35c89270acdd Starting session with SessionId: ecs-execute-command-0730176e68438b66d root@ip-172-19-0-73:/# We have more commands available for inspecting the service, understanding the status of a deployment, etc. As developers need specific operations we can quickly develop them since they are wrappers over AWS\u0026rsquo;s API.\nLooking forward We understand that what we built here is rather simple but we believe it solves a big pain we were facing and lays a very strong foundation for future requirements we may have. One of the big discussions that we are going to have in the short term is whether we want to stick with ECS or not. While it certainly has good functionality and is the right tool for many use cases, we are starting to hit some of its limitations and they are starting to hurt. When we started with this idea we had a short conversation on whether we wanted to stick with ECS or not, but since we were (and are) short staffed with a lot of important things to tackle we decided to leave that discussion for the future. While this may change some details of this process we believe that the concepts and conventions will remain.\nWhat are our next steps? For now we are focused on other projects, the biggest one was mentioned previously and is the breaking of the monolith. We are an infrastructure and Engineering Effectiveness team which is why we are the owners of it. We believe that as we have more services we will encounter use cases that will force us to adopt and develop new and exciting things, so we can\u0026rsquo;t wait!\nRemarks Thank you for sticking until the end! We have many exciting projects going on and more blog posts may come to this blog. While I\u0026rsquo;m the one writing this, I\u0026rsquo;m not by any means the only one that should receive credit for this work. I wanted to mention Claudio Martinez and the entire Infra/SRE team at Lemon Cash because without them this blog would not exist üòÅ\n","permalink":"https://blog.matiaspan.dev/posts/how-services-are-provisioned-and-deployed-at-lemoncash/","summary":"Lemon Cash is a crypto startup based in Argentina that operates digital wallets, card payments and more. In this blog post I describe how we implemented a solution for provisioning, deploying and operating services that empowers developers to ship more quickly. This solution is not a fully fledged platform but it provides similar benefits by standing on top of strongly defined concepts and conventions. What we implemented only makes sense in the context in which the company was in, so I will start by explaining what that context was.","title":"How services are provisioned, deployed and operated at Lemon Cash"},{"content":"The code used for this test can be found here.\nTL;DR: when looking into overall performance and usability, json-iter is the clear winner. It gives roughly a 4x improvement over encoding/json and 1.2x over the second most performant option. It is also extremely easy to use. You simply need to import it, define a global variable (that you can call json to make it even easier) and then use it like you would use encoding/json.\nServices that serve OpenRTB requests are typically under heavy load and have very strict latency constraints. In this scenario a fast json parser is highly desirable. In the Go ecosystem there are many json parsing libraries that claim to have better performance than encoding/json. The following list are the libraries that I will consider for this test:\neasyjson jsonparser simdjson-go json-iter encoding/json Performance Lets go directly to the performance results. Unmarshaling a standard OpenRTB request with a single imp object yields the following numbers:\ngoos: darwin goarch: amd64 pkg: github.com/matipan/openrtb BenchmarkRequest_UnmarshalJSON/json-iter-12 255331\t4730 ns/op\t1328 B/op\t50 allocs/op BenchmarkRequest_UnmarshalJSON/easyjson-12 197110\t5695 ns/op\t1432 B/op\t28 allocs/op BenchmarkRequest_UnmarshalJSON/jsonparser-12 128836\t9360 ns/op\t5528 B/op\t94 allocs/op BenchmarkRequest_UnmarshalJSON/encoding/json-12 70920\t17180 ns/op\t1752 B/op\t49 allocs/op BenchmarkRequest_UnmarshalJSON/simdjson-go-12 46688\t26658 ns/op\t115388 B/op\t48 allocs/op PASS ok github.com/matipan/openrtb\t7.176s This shows that the fastest option of all is json-iter. It is 1.2x compared to the second, easyjson, 2x compared to jsonparser and about 4x compared to encoding/json. Surprisingly, simdjson is falling way behind being 6x slower than json-iter. I have not done an in depth analysis of why simdjson is this slow. If I do, I will post the results here.\nPerformance-wise json-iter is the clear winner.\nUsability In terms of usability each library requires something different compared to encoding/json:\neasyjson: requires code generation. I\u0026rsquo;m ok with this to be honest, but it adds yet another command that all developers need to be aware of when modifying your structures. jsonparser: requires a lot of manual parsing to get something working. simdjson-go: requires even more manual parsing than jsonparser. json-iter: requires importing a new library and, if you want to use the fastest option, adding a global variable that has to be used for every unmarshal/marshal call. encoding/json: the familiar encode/decode and marshal/unmarshal API. Below you can see an in depth explanation of how to use each of the libraries shown above. But in my opinion usability-wise json-iter wins again.\nUsing json-iter To use json-iter you can simply import the library and define a global variable using the fastest option:\nimport jsoniter \u0026#34;github.com/json-iterator/go\u0026#34; var json = jsoniter.ConfigFastest Once you\u0026rsquo;ve done that you can use the familiar Marshal/Unmarshal and Encode/Decode API.\nUsing easyjson To use easyjson you need to first install the CLI they provide:\ngo get -u github.com/mailru/easyjson/... With this CLI you can generate code that parses JSON into and out of the structure. First, add the following directive to your structure:\n//easyjson:json type Request struct { ... } With that you can run this command that will generate functions that match the signature of encoding/json.Marshaler and encoding/json.Unmarshaler:\neasyjson -byte -pkg Using jsonparser There are many ways to parse a JSON using the jsonparser library. I went with an approach that has a lot of code but most of it is boiler plate, which means that adding new fields is relatively straight forward and does not require a lot of modifications. We start by defining all the fields that the OpenRTB request will have and mapping their IDs to the path were it can be found:\ntype fieldIdx iota const ( fieldDevice fieldIdx = iota fieldImp fieldApp fieldId fieldAt fieldBcat fieldBadv fieldBapp fieldRegs ) type rtbFieldDef struct { idx fieldIdx path []string } var ( reqFields = []rtbFieldDef{ {fieldDevice, []string{\u0026#34;device\u0026#34;}}, {fieldImp, []string{\u0026#34;imp\u0026#34;}}, {fieldApp, []string{\u0026#34;app\u0026#34;}}, {fieldId, []string{\u0026#34;id\u0026#34;}}, {fieldAt, []string{\u0026#34;at\u0026#34;}}, {fieldBcat, []string{\u0026#34;bcat\u0026#34;}}, {fieldBadv, []string{\u0026#34;badv\u0026#34;}}, {fieldBapp, []string{\u0026#34;bapp\u0026#34;}}, {fieldRegs, []string{\u0026#34;regs\u0026#34;}}, } reqPaths = rtbBuildPaths(reqFields) ) func rtbBuildPaths(fields []rtbFieldDef) [][]string { ret := make([][]string, 0, 10) for _, f := range fields { ret = append(ret, f.path) } return ret } Once we\u0026rsquo;ve defined the fields for this top level object we can write the parsing function. This function basically iterates over the JSON one key at a time. For each key it finds it tries to map it to one of the keys we defined in the reqPaths variable. If it finds a match then it sets the value to the corresponding fields on the structure:\nfunc (r *Request) UnmarshalJSONReq(b []byte) error { jsonparser.EachKey(b, func(idx int, value []byte, vt jsonparser.ValueType, err error) { r.setField(idx, value, vt, err) }, reqPaths...) return nil } func (data *Request) setField(idx int, value []byte, _ jsonparser.ValueType, _ error) { switch fieldIdx(idx) { case fieldDevice: data.Device = \u0026amp;Device{} data.Device.UnmarshalJSONReq(value) case fieldImp: data.Imps = []*Imp{} jsonparser.ArrayEach(value, func(arrdata []byte, dataType jsonparser.ValueType, offset int, err error) { imp := \u0026amp;Imp{} if err := imp.UnmarshalJSONReq(value); err != nil { return } data.Imps = append(data.Imps, imp) }) case fieldApp: data.App = \u0026amp;App{} data.App.UnmarshalJSONReq(value) case fieldId: data.ID = string(value) case fieldAt: data.At, _ = strconv.ParseInt(string(value), 10, 64) case fieldBcat: data.BCat = []string{} jsonparser.ArrayEach(value, func(arrdata []byte, dataType jsonparser.ValueType, offset int, err error) { data.BCat = append(data.BCat, string(value)) }) case fieldBadv: data.BAdv = []string{} jsonparser.ArrayEach(value, func(arrdata []byte, dataType jsonparser.ValueType, offset int, err error) { data.BAdv = append(data.BAdv, string(value)) }) case fieldBapp: data.BApp = []string{} jsonparser.ArrayEach(value, func(arrdata []byte, dataType jsonparser.ValueType, offset int, err error) { data.BApp = append(data.BApp, string(value)) }) case fieldRegs: data.Regs = \u0026amp;Regs{} data.Regs.UnmarshalJSONReq(value) } } With this you can start parsing the top level object. However, if you look at the fieldApp key for example you can see that we are calling an UnmarshalJSONReq function of the App object. This structure essentially does the same thing than the top level one. It first defines the list of fields that we care about and their corresponding paths and then implements the function that iterates over each key setting the corresponding values:\nconst ( fieldAppName fieldIdx = iota fieldAppPubId fieldAppBundle fieldAppLanguage fieldAppId fieldExtDevUserId ) var ( appFields = []rtbFieldDef{ {fieldAppName, []string{\u0026#34;name\u0026#34;}}, {fieldAppPubId, []string{\u0026#34;publisher\u0026#34;, \u0026#34;id\u0026#34;}}, {fieldAppBundle, []string{\u0026#34;bundle\u0026#34;}}, {fieldAppLanguage, []string{\u0026#34;content\u0026#34;, \u0026#34;language\u0026#34;}}, {fieldAppId, []string{\u0026#34;id\u0026#34;}}, {fieldExtDevUserId, []string{\u0026#34;ext\u0026#34;, \u0026#34;devuserid\u0026#34;}}, } appPaths = rtbBuildPaths(appFields) ) func (a *App) setField(idx int, value []byte, _ jsonparser.ValueType, _ error) { switch fieldIdx(idx) { case fieldAppName: a.Name = string(value) case fieldAppPubId: a.Publisher.ID = string(value) case fieldAppBundle: a.Bundle = string(value) case fieldAppLanguage: a.Content.Language = string(value) case fieldAppId: a.ID = string(value) case fieldExtDevUserId: a.Ext.Devuserid = string(value) } } func (a *App) UnmarshalJSONReq(b []byte) error { a.Publisher = \u0026amp;Publisher{} a.Content = \u0026amp;AppContent{} a.Ext = \u0026amp;AppExt{} jsonparser.EachKey(b, func(idx int, value []byte, vt jsonparser.ValueType, err error) { a.setField(idx, value, vt, err) }, appPaths...) return nil } If you want to add a new object to the structure you need to implement all this boiler plate. However, if you just want to add a new field to an existing object the change is relatively straight forward. This is why jsonparser in terms of usability is worst than json-iter but still better than simdjson-go.\nUsing simdjson-go There is a lot of code involved when parsing JSON with simdjson-go. Here I went with an approach that basically requires you to add more parsing code every time you add a new field. We could implement this with reflection like encoding/json does but that would hurt the performance even more.\nImplementing a parser for simdjson requires one to start iterating over the tape that the library generated and map each field according to its type. We start by parsing the top level object and identifying it as a simdjson.TypeObject:\nfunc (r *Request) UnmarshalJSONSimd(b []byte) error { parsed, err := simdjson.Parse(b, nil) if err != nil { return err } var ( iter = parsed.Iter() obj = \u0026amp;simdjson.Object{} tmp = \u0026amp;simdjson.Iter{} ) for { typ := iter.Advance() switch typ { case simdjson.TypeRoot: if typ, tmp, err = iter.Root(tmp); err != nil { return err } switch typ { case simdjson.TypeObject: if obj, err = tmp.Object(obj); err != nil { return err } return r.parse(tmp, obj) } default: return nil } } } Within the simdjson.TypeObject switch we can start parsing the OpenRTB request, but the request has many internal objects and each of them can have a different type. This means that for every one of those types we need to parse it separately. To keep this example brief we will only parse two types, Object and Array:\nfunc (r *Request) parse(tmp *simdjson.Iter, obj *simdjson.Object) error { arr := \u0026amp;simdjson.Array{} for { name, t, err := obj.NextElementBytes(tmp) if err != nil { return err } if t == simdjson.TypeNone { break } switch t { case simdjson.TypeObject: if err := r.parseObject(name, tmp); err != nil { return err } case simdjson.TypeArray: if _, err := tmp.Array(arr); err != nil { return err } if err := r.parseArray(name, tmp, arr); err != nil { return err } } } return nil } Lets double click on the parsing of an Object. Within an OpenRTB request we have many different objects, like device and app. Each object will require its own parsing function that will map each available key to the corresponding value of the structure. For example, parsing the device object would look something like this:\nfunc (d *Device) parse(iter *simdjson.Iter, obj *simdjson.Object) error { for { name, t, err := obj.NextElementBytes(iter) if err != nil { return err } if t == simdjson.TypeNone { return nil } switch t { case simdjson.TypeInt: n, err := iter.Int() if err != nil { return err } switch { case bytes.Compare(name, hKey) == 0: d.H = n case bytes.Compare(name, wKey) == 0: d.W = n case bytes.Compare(name, dtKey) == 0: d.DeviceType = n case bytes.Compare(name, ctKey) == 0: d.ConnectionType = n } case simdjson.TypeString: b, err := iter.StringBytes() if err != nil { return err } switch { case bytes.Compare(name, ipKey) == 0: d.IP = string(b) case bytes.Compare(name, uaKey) == 0: d.UA = string(b) case bytes.Compare(name, osKey) == 0: d.OS = string(b) case bytes.Compare(name, osvKey) == 0: d.OSV = string(b) case bytes.Compare(name, ifaKey) == 0: d.IFA = string(b) case bytes.Compare(name, hwvKey) == 0: d.HWV = string(b) case bytes.Compare(name, modelKey) == 0: d.Model = string(b) case bytes.Compare(name, dntKey) == 0: d.DNT = string(b) case bytes.Compare(name, langKey) == 0: d.Language = string(b) } } } } In this example you can see how tedious it would be to add a new field or top level object. Which is why from a usability point of view, simdjson-go is way behind.\nConclusion When looking into overall performance and usability, json-iter is the clear winner. It gives roughly a 4x improvement over encoding/json and 1.2x over the second most performant option. It is also extremely easy to use. You simply need to import it, define a global variable (that you can call json to make it even easier) and then use it like you would use encoding/json. On top of that, the community around json-iter seems to be really active. And this library is supported on many different languages.\n","permalink":"https://blog.matiaspan.dev/posts/fast-json-parsing-for-open-rtb/","summary":"The code used for this test can be found here.\nTL;DR: when looking into overall performance and usability, json-iter is the clear winner. It gives roughly a 4x improvement over encoding/json and 1.2x over the second most performant option. It is also extremely easy to use. You simply need to import it, define a global variable (that you can call json to make it even easier) and then use it like you would use encoding/json.","title":"Fast JSON parsing in Go for OpenRTB"},{"content":"There is a new term in town that has been making its way to all of us for a couple of years now: Serverless. When I first heard this I was kind of confused as to what it meant. Everybody seemed to have an opinion about it but there were no real answers. The following definition from serverless-stack helped me clarify a bit:\nServerless computing (or serverless for short), is an execution model where the cloud provider is responsible for executing a piece of code by dynamically allocating the resources. And only charging for the amount of resources used to run the code.\nIt also claims the following:\nWhile serverless abstracts the underlying infrastructure away from the developer, servers are still involved in executing our functions.\nEssentially we want to write our code without worrying where it will be executed, how it will be scaled and how it will exposed to the rest of the world. This talk by Kelsey Hightower does an amazing job in showing off what serverless is all about.\nIn this post I\u0026rsquo;ll go through how to implement our own Serverless function using OpenFaaS. We are going to create a small program that receives an image or a URL and returns the same image with all the faces marked with a green rectangle. We\u0026rsquo;ll do this using OpenCV and a neural network.\nPrerequisites For this tutorial I will be using GCP\u0026rsquo;s (Google Cloud Platform) managed Kubernetes service called GKE to deploy OpenFaaS and the face detection program will be built using Go and GoCV. So be sure you meet with the following requirements:\nGo installed on your machine. If you want to test the program locally you\u0026rsquo;ll either need to install GoCV or use one of their available docker images. A Kubernetes cluster with kubectl set up. If you want you can use GCP\u0026rsquo;s 300USD free credit. We won\u0026rsquo;t be using much of it. You can follow this tutorial to set it up. You can use Minikube for this if you prefer to test it all locally. I haven\u0026rsquo;t actually followed that blog post so I\u0026rsquo;m not sure if it\u0026rsquo;s still working. OpenFaaS We already said that while serverless abstracts the underlying infrastructure from your code that doesn\u0026rsquo;t mean there is no infrastructure behind. There are tools that different cloud providers have such as Google\u0026rsquo;s Cloud Functions, Microsoft\u0026rsquo;s Azure Functions and AWS Lambda that allow us to write our serverless functions. But today we are going to be looking at one called OpenFaaS.\nOpenFaaS is a platform that tries to Make Serverless Functions Simple (and it does that very well). It can run on top of something like Kubernetes, Docker Swarm, Fargate and others. It makes life easier for developers and operators because of its ease of use and amazing CLI. Check out the documentation. It\u0026rsquo;s very extensive and has all the information you\u0026rsquo;ll need.\nBefore we start writing any code, let\u0026rsquo;s get it up and running with OpenFaaS. Since you already have a Kubernetes cluster, follow this tutorial to deploy it using helm. Once you have it up and running install the faas-cli on your machine and authenticate to your OpenFaaS installation.\nFunctions and templates All functions in OpenFaaS belong to a template. There are many templates for different languages and tools already provided. But if none of those meet your needs, it is very easy to create a new one. This is what we\u0026rsquo;ll be doing in this article.\nWriting our function To perform the face detection we are going to use GoCV and a pre-trained Caffe neural network. This means that our function needs to have OpenCV installed and the model+config files of the neural network. We could use the dockerfile template and install our dependencies there, but I like the idea of having a gocv template with built-in models for anyone who wants to make use of it. Here is the template I created in case you want to check it out.\nLet\u0026rsquo;s start writing our function! The users will provide the image they want to perform the face detection on as a URL. The function will:\nDo a GET request to that URL and check if it\u0026rsquo;s an image and if the image is of the supported content types (jpg, jpeg and png). If it\u0026rsquo;s not then we simply return an error. If it is valid we need to: decode the image, run a pass through the neural network, draw rectangles around all of the faces that were found, encode the image and return the results. The code to do the detection was extracted from this example of GoCV. I had to make a few minor changes to parse the downloaded image and encode it before returning it, but the important parts are the same. Download the function\u0026rsquo;s code from Github and let\u0026rsquo;s deploy it using the faas cli. Once you\u0026rsquo;ve downloaded the function go to the stack.yml file and change the following fields:\nprovider \u0026gt; gateway: set your openfaas gateway URL functions \u0026gt; face-finder \u0026gt; image: change matipan for your own docker hub username. Now we simply have to deploy the function, this is as simple as running: faas up. This command will build the image, push it to the docker registry and deploy it on openfaas. Once the function is deployed head over to the OpenFaaS dashboard and select your face-finder function. Provide a URL(like this one) to an image then select the Download option and hit invoke. This should download an image onto your machine, if it all worked it should be something like this:\nPretty neat, right?\nIf you are interested in getting into details on how this was implemented you can check out the function/handler.go file. It is less than 100 lines of code and is quite straightforward.\nIf you have any questions feel free to contact me on Twitter!\n","permalink":"https://blog.matiaspan.dev/posts/writing-a-face-detection-function-for-openfaas/","summary":"There is a new term in town that has been making its way to all of us for a couple of years now: Serverless. When I first heard this I was kind of confused as to what it meant. Everybody seemed to have an opinion about it but there were no real answers. The following definition from serverless-stack helped me clarify a bit:\nServerless computing (or serverless for short), is an execution model where the cloud provider is responsible for executing a piece of code by dynamically allocating the resources.","title":"Writing a face detection function for OpenFaaS"},{"content":"Disclaimer: this blog post is just a port of Adrian\u0026rsquo;s tutorial at pyimagesearch where he shows how to track a ball using Python and OpenCV. I only changed a few things here and there and rewrote it using Go and GoCV. So all the credit should go to him\nI always prefer to start by showing what you\u0026rsquo;ll get if you stick to the end, so here it goes:\nHooked? Awesome.\nHere are the two main sections of this post:\nFind the range of pixel values of an object in the HSV color space. Detect the presence of that object using thresholding techniques and track its movements Range of values in the HSV color space The HSV(hue, saturation, value) color space is a different way of representing colors:\nhue: this channel models the color type, this makes it useful for image processing tasks where we need to process objects based on its color. saturation: represents the different shades of gray or if it\u0026rsquo;s fully saturated, meaning there is no white component. value: describes the intensity or brightness of the color. Here is an image showing the HSV cylinder:\nLets write a program that will allow us to determine the range of pixel values of our object, these values will be necessary to track it later. Create a new folder somewhere and add a main.go file there with the following:\npackage main import ( \u0026#34;gocv.io/x/gocv\u0026#34; ) func main() { wi := gocv.NewWindow(\u0026#34;normal\u0026#34;) wt := gocv.NewWindow(\u0026#34;threshold\u0026#34;) wt.ResizeWindow(600, 600) wt.MoveWindow(0, 0) wi.MoveWindow(600, 0) wi.ResizeWindow(600, 600) lh := wi.CreateTrackbar(\u0026#34;Low H\u0026#34;, 360/2) hh := wi.CreateTrackbar(\u0026#34;High H\u0026#34;, 255) ls := wi.CreateTrackbar(\u0026#34;Low S\u0026#34;, 255) hs := wi.CreateTrackbar(\u0026#34;High S\u0026#34;, 255) lv := wi.CreateTrackbar(\u0026#34;Low V\u0026#34;, 255) hv := wi.CreateTrackbar(\u0026#34;High V\u0026#34;, 255) video, _ := gocv.OpenVideoCapture(0) img := gocv.NewMat() for { video.Read(\u0026amp;img) gocv.CvtColor(img, \u0026amp;img, gocv.ColorBGRToHSV) thresholded := gocv.NewMat() gocv.InRangeWithScalar(img, gocv.Scalar{Val1: getPosFloat(lh), Val2: getPosFloat(ls), Val3: getPosFloat(lv)}, gocv.Scalar{Val1: getPosFloat(hh), Val2: getPosFloat(hs), Val3: getPosFloat(hv)}, \u0026amp;thresholded) wi.IMShow(img) wt.IMShow(thresholded) if wi.WaitKey(1) == 27 || wt.WaitKey(1) == 27 { break } } } func getPosFloat(t *gocv.Trackbar) float64 { return float64(t.GetPos()) } In the first 6 lines of the main function we create two different windows, one for our normal images and one for our filtered images. We also resize them so that they show up side by side.\nAfter that we create 6 different trackbars. These trackbars will allow you to control the high and low pixel values of each component of the HSV color space. You are going to need to play around with those bars in order to determine the true values of your object.\nNow for the important part. Each new frame we read inside the for loop gets converted to HSV using the CvtColor function. Once we have that we want to apply pixel-thresholding to our image using the InRangeWithScalar function. This function will leave us with a binary image where only the pixels that are in the ranges specified by the trackbars will be displayed.\nFinally we simply display both images.\nTo run this program be sure that your camera is in fact identified by the 0 index, if not then change that number to the one appropriate to your setup.\nIf all went well the following two windows should appear:\nNow it\u0026rsquo;s time to start playing around with those 6 trackbars you can see there. Each one of them modifies the high and low value of each of the three HSV components. Note that the values required to find the object you want will vary depending on the lighting of the room and the camera you are using.\nLets show an example of the values I needed to set in order to find the hoodie I was wearing:\nOnce you have those values make sure to write them down since you\u0026rsquo;ll need them for the next part.\nDetect and track the object In order to draw that red line you saw on the video we are going to need to implement a new structure. If you pay close attention you\u0026rsquo;ll see that the line fades away after a fixed number of points have been drawn. So, in order to only draw the points that we care about we are going to build our own queue that discards old elements when the buffer gets full:\npackage queue import \u0026#34;image\u0026#34; // Queue is a fixed-size queue that discards old // elements once it reached the maximum size. type Queue struct { data []image.Point size int } // New creates a new Queue with the specified size. func New(size uint) *Queue { return \u0026amp;Queue{ data: []image.Point{}, size: int(size), } } // Clear clears all elements in the queue. func (q *Queue) Clear() { q.data = []image.Point{} } // Push pushes a new element into the queue. func (q *Queue) Push(p image.Point) { if len(q.data) == q.size { q.data = q.data[1 : q.size-1] } q.data = append(q.data, p) } // Range iterates over the elements of the queue // calling f for each element. func (q *Queue) Range(f func(p image.Point)) { for _, p := range q.data { f(p) } } // RangePrevious iterates over the elements of the queue // calling f for each pair of previous-current elements. func (q *Queue) RangePrevious(f func(current image.Point, previous image.Point)) { for i := 1; i \u0026lt; len(q.data); i++ { f(q.data[i], q.data[i-1]) } } The basic idea of this structure is to basically provide a way to push data into the queue discarding old elements once we reach the maximum size we specified. It also provides the RangePrevious function that allows the user of the queue to range over the data using both the current item and the previous item, you\u0026rsquo;ll see in a bit why we need this. You can find this package here.\nTime to actually track the object. Open up a new main.go file and we\u0026rsquo;ll start by initializing the basic data structures we\u0026rsquo;ll need:\npackage main import ( \u0026#34;image\u0026#34; \u0026#34;image/color\u0026#34; \u0026#34;github.com/matipan/computer-vision/queue\u0026#34; \u0026#34;gocv.io/x/gocv\u0026#34; ) var ( lhsv = gocv.Scalar{Val1: 49, Val2: 89, Val3: 0} hhsv = gocv.Scalar{Val1: 109, Val2: 255, Val3: 255} size = image.Point{X: 600, Y: 600} wt = gocv.NewWindow(\u0026#34;thersholded\u0026#34;) wi = gocv.NewWindow(\u0026#34;images\u0026#34;) img = gocv.NewMat() mask = gocv.NewMat() frame = gocv.NewMat() hsv = gocv.NewMat() kernel = gocv.NewMat() ) func main() { defer close() wt.ResizeWindow(600, 600) wt.MoveWindow(0, 0) wi.MoveWindow(600, 0) wi.ResizeWindow(600, 600) video, _ := gocv.OpenVideoCapture(0) defer video.Close() queue := queue.New(40) for { video.Read(\u0026amp;img) if imShow() { break } } } func imShow() bool { wi.IMShow(img) wt.IMShow(mask) return wi.WaitKey(1) == 27 || wt.WaitKey(1) == 27 } func close() { defer img.Close() defer mask.Close() defer frame.Close() defer hsv.Close() defer kernel.Close() defer wi.Close() defer wt.Close() } Lets start from the beginning. First, we are defining the low and high pixel values of our object in the HSV color space:\nlhsv = gocv.Scalar{Val1: 49, Val2: 89, Val3: 0} hhsv = gocv.Scalar{Val1: 109, Val2: 255, Val3: 255} Modify those values to match the ones you found before. lhsv represents the low values and hhsv represents the high values. Val1, Val2 and Val3 represent each HSV component in order.\nNext we initialize all the Mats we are going to need to apply our filters and the windows we\u0026rsquo;ll use to display the images.\nWe also initialized our queue with a size of 40. This means that the maximum length of our line will be 40. If you want a bigger line simply increase that number. Finally, we read new frames from our webcam and display them using our convenience imShow function. If you want to quit the program you can hit escape at any time. Note that this program wont compile since we have not used our queue yet.\nTo each new frame we read we need to apply a few filters before we can find our object:\ngocv.Flip(img, \u0026amp;img, 1) gocv.Resize(img, \u0026amp;img, size, 0, 0, gocv.InterpolationLinear) gocv.GaussianBlur(img, \u0026amp;frame, image.Point{X: 21, Y: 21}, 0, 0, gocv.BorderReflect101) gocv.CvtColor(frame, \u0026amp;hsv, gocv.ColorBGRToHSV) Flip flips the image vertically, this is not necessary but I think it looks better. After that we Resize our image, apply a Gaussian filter to blur the image and convert its color to HSV with CvtColor.\nNow that our image is filtered we can look for our object:\ngocv.InRangeWithScalar(hsv, lhsv, hhsv, \u0026amp;mask) gocv.Erode(mask, \u0026amp;mask, kernel) gocv.Dilate(mask, \u0026amp;mask, kernel) cnt := bestContour(mask, 2000) InRangeWithScalar finds all pixels in our image that are between the range of pixel values defined by lhsv and hhsv. After that we perform an Erosion and Dilation to expand the pixels that were in that range. Finally we find the biggest contour in our image. If you read my previous blog post you\u0026rsquo;ll be familiar with the bestContour function. But in case you didn\u0026rsquo;t here it is:\n// bestContour obtains the biggest contour in the frame(provided is bigger) // than the minArea. func bestContour(frame gocv.Mat, minArea float64) []image.Point { cnts := gocv.FindContours(frame, gocv.RetrievalExternal, gocv.ChainApproxSimple) var ( bestCnt []image.Point bestArea = minArea ) for _, cnt := range cnts { if area := gocv.ContourArea(cnt); area \u0026gt; bestArea { bestArea = area bestCnt = cnt } } return bestCnt } This function will return the biggest contour found in the image as long as the area of that contour is bigger than minArea.\nOnce we have our contour we can draw the rectangle around it, this can be done easily with BoundingRect and Rectangle:\nrect := gocv.BoundingRect(cnt) gocv.Rectangle(\u0026amp;img, rect, color.RGBA{G: 255}, 2) To draw the line that follows the movement of the object we first need to get the center of the rectangle:\n// middle calculates the middle x and y of a rectangle. func middle(rect image.Rectangle) (x int, y int) { return (rect.Max.X-rect.Min.X)/2 + rect.Min.X, (rect.Max.Y-rect.Min.Y)/2 + rect.Min.Y } Finally we are going to use our queue to push the center coordinates of the rectangle, range over all the elements of the queue and display the connection between each of the points using the Line function:\nrect := gocv.BoundingRect(cnt) gocv.Rectangle(\u0026amp;img, rect, color.RGBA{G: 255}, 2) x, y := middle(rect) queue.Push(image.Point{X: x, Y: y}) queue.RangePrevious(func(c image.Point, p image.Point) { gocv.Line(\u0026amp;img, p, c, color.RGBA{R: 255}, 2) }) Lets put everything together now:\npackage main import ( \u0026#34;image\u0026#34; \u0026#34;image/color\u0026#34; \u0026#34;github.com/matipan/computer-vision/queue\u0026#34; \u0026#34;gocv.io/x/gocv\u0026#34; ) var ( rcolor = color.RGBA{G: 255, A: 255} lcolor = color.RGBA{R: 255, A: 255} lhsv = gocv.Scalar{Val1: 49, Val2: 89, Val3: 0} hhsv = gocv.Scalar{Val1: 109, Val2: 255, Val3: 255} size = image.Point{X: 600, Y: 600} blur = image.Point{X: 11, Y: 11} wt = gocv.NewWindow(\u0026#34;thersholded\u0026#34;) wi = gocv.NewWindow(\u0026#34;images\u0026#34;) img = gocv.NewMat() mask = gocv.NewMat() frame = gocv.NewMat() hsv = gocv.NewMat() kernel = gocv.NewMat() ) func main() { defer close() wt.ResizeWindow(600, 600) wt.MoveWindow(0, 0) wi.MoveWindow(600, 0) wi.ResizeWindow(600, 600) video, _ := gocv.OpenVideoCapture(0) defer video.Close() queue := queue.New(40) for { if !video.Read(\u0026amp;img) { break } gocv.Flip(img, \u0026amp;img, 1) gocv.Resize(img, \u0026amp;img, size, 0, 0, gocv.InterpolationLinear) gocv.GaussianBlur(img, \u0026amp;frame, blur, 0, 0, gocv.BorderReflect101) gocv.CvtColor(frame, \u0026amp;hsv, gocv.ColorBGRToHSV) gocv.InRangeWithScalar(hsv, lhsv, hhsv, \u0026amp;mask) gocv.Erode(mask, \u0026amp;mask, kernel) gocv.Dilate(mask, \u0026amp;mask, kernel) cnt := bestContour(mask, 2000) if len(cnt) == 0 { queue.Clear() if imShow() { break } continue } rect := gocv.BoundingRect(cnt) gocv.Rectangle(\u0026amp;img, rect, rcolor, 2) x, y := middle(rect) queue.Push(image.Point{X: x, Y: y}) queue.RangePrevious(func(c image.Point, p image.Point) { gocv.Line(\u0026amp;img, p, c, lcolor, 2) }) if imShow() { break } } } func imShow() bool { wi.IMShow(img) wt.IMShow(mask) return wi.WaitKey(1) == 27 || wt.WaitKey(1) == 27 } // bestContour obtains the biggest contour in the frame(provided is bigger) // than the minArea. func bestContour(frame gocv.Mat, minArea float64) []image.Point { cnts := gocv.FindContours(frame, gocv.RetrievalExternal, gocv.ChainApproxSimple) var ( bestCnt []image.Point bestArea = minArea ) for _, cnt := range cnts { if area := gocv.ContourArea(cnt); area \u0026gt; bestArea { bestArea = area bestCnt = cnt } } return bestCnt } // middle calculates the middle x and y of a rectangle. func middle(rect image.Rectangle) (x int, y int) { return (rect.Max.X-rect.Min.X)/2 + rect.Min.X, (rect.Max.Y-rect.Min.Y)/2 + rect.Min.Y } func close() { defer wi.Close() defer wt.Close() defer img.Close() defer mask.Close() defer frame.Close() defer hsv.Close() defer kernel.Close() } Remember to set your own values for lhsv and hhsv.\nIf all went well after you run this program and start moving the object you\u0026rsquo;ll see something similar to this:\nYAS! High five yourself!!\nConclusion In this blog post we tweaked a bit Adrian\u0026rsquo;s ball tracking blog post to use the functions that we have available in GoCV. If you follow his blog post you\u0026rsquo;ll see that he used a function called minEnclosingCircle. This function allows you to get a circle around your contour that you can then draw on the image. The problem is that GoCV has not that implemented that function yet, although they have it on their Roadmap. This is why I decided to simply draw a rectangle but still do the fun part(at least for me) of drawing the line that follows the object.\nHope you enjoyed this and thanks for reading! Until next time!\n","permalink":"https://blog.matiaspan.dev/posts/box-tracking-with-gocv/","summary":"Disclaimer: this blog post is just a port of Adrian\u0026rsquo;s tutorial at pyimagesearch where he shows how to track a ball using Python and OpenCV. I only changed a few things here and there and rewrote it using Go and GoCV. So all the credit should go to him\nI always prefer to start by showing what you\u0026rsquo;ll get if you stick to the end, so here it goes:\nHooked? Awesome.","title":"Tracking color objects with GoCV"},{"content":"The title says it, motion tracking turret using only Go, ready to have some func?\nThis blog post will be divided into three main parts: Motion detection with GoCV, Controlling servo motors with Gobot and Putting it all together. In the first part, we build a lightweight motion detection algorithm that can run on low-power devices such as the RPi. In the second part we will show how to control servo motors with Gobot from the Raspberry Pi and in the last part we\u0026rsquo;ll explain how to go from detecting an object on an image to telling exactly the angles in which the servos need to move in order to track it.\nFor this blog post I assume that you already know Go. If you don\u0026rsquo;t but want to learn, there are lots of resources out there. Anyway, a good and simple place to start is the Go tour.\nThis entire project is hosted on Github.\nBefore we start let me show you what you will get once you are done if you follow this series of blog posts: Just kidding, this is what you will have(for real this time): Looks lethal, right? Lets dive in and see how we can build this sophisticated piece of machinery. Starting of with the list of things you\u0026rsquo;ll need to follow this tutorial:\nTwo MG90 tower pro micro servos: MercadoLibre - Amazon. One Raspberry Pi 3 B+ with Raspbian installed: MercadoLibre - Amazon. A bunch of cables and a breadboard: MercadoLibre - Amazon. One 5V and 2A power source: MercadoLibre - Amazon. Either a 3D printer or a shop that can print the case(although you could build your own) C270 Logitech web camera is what I used, but any other model should work: MercadoLibre - Amazon. 5V laser(it\u0026rsquo;s not the best fit but we\u0026rsquo;ll probably update it in the future): MercadoLibre - Amazon. Bonus: soldering iron if you want to build your own PCB or solder some cables Note: if you want to use different things go ahead, but I don\u0026rsquo;t guarantee they will work with the same code.\nMotion detection with GoCV You might ask what is this GoCV thing he\u0026rsquo;s talking about? Well, first lets explain what OpenCV is. OpenCV is a library for computer vision, better yet, is the library for computer vision. It has a whole lot of functions, types and interfaces that allow us to manipulate images by applying already implemented filters and image-manipulation algorithms. It\u0026rsquo;s a really cool project, you should definitely check it out.\nThe thing is that OpenCV is implemented in C++ and it has interfaces for Java and Python. But don\u0026rsquo;t be afraid my gopher-friend, the Hybridgroup has got us. Along with other libraries they implemented a Go wrapper for OpenCV. They did this by using CGo to call C code from Go code and that C code calls the corresponding C++ code of OpenCV. It\u0026rsquo;s really efficient and works pretty well. Lucky for us, this wrapper is really fast and most of the OpenCV functionality is already there.\nInstallation We are going to be running this program on a Raspberry Pi but since Go is nice and lets us do cross-compilation we will only install GoCV and OpenCV on our development machine. So head over to GoCV\u0026rsquo;s how to install section and follow the steps required for your platform. It\u0026rsquo;ll take a while so I\u0026rsquo;ll wait here till you are back.\nExtra: if you want to debug and change the code while testing in the Pi you can install Go and after that install GoCV for Raspbian following this instructions.\nMotion detection algorithm You back? Awesome. Lets explain just a bit about how the motion detection algorithm works so that you understand what it\u0026rsquo;s going on and can tweak it to your needs.\nSince we are going to run this on the Pi, we are not going to use a fancy already trained neural network with near zero error margin. Instead we will do something relatively simple:\nWhen the program starts we take a picture, convert it to gray and blur it using a Guassian filter. That first frame will be considered our background so try not be there when it starts. Constantly read new frames, perform the same conversion we did before, compute the absolute difference between the first frame and the current frame, apply a threshold to the image so that we create a binary image where the area of movement will look really bright. Finally, dilate the resulting binary image and find the biggest contour. We will consider that contour our area of movement. Once all these filters were applied our image will look something like this: Time to implement this using GoCV.\nFirst, lets write a program that opens the feed of a camera to read images, stores the first frame and starts reading new frames non stop while showing them on a window(error handling for now is out of scope). I\u0026rsquo;m using device 0, to check which devices you have available you can do ls /dev/video*:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;gocv.io/x/gocv\u0026#34; ) var ( firstFrame = gocv.NewMat() frame = gocv.NewMat() ) func main() { window := gocv.NewWindow(\u0026#34;Motion\u0026#34;) video, _ := gocv.OpenVideoCapture(0) video.Read(\u0026amp;firstFrame) for { video.Read(\u0026amp;frame) window.IMShow(frame) if window.WaitKey(1) == 27 { break } } video.Close() window.Close() firstFrame.Close() frame.Close() } If you run this program you\u0026rsquo;ll basically be streaming from your camera to the window that just opened. To quit this window you can hit escape, that is what that if window.WaitKey(1) == 27 is doing.\nSince each frame we read(including the first one) needs to be converted to a gray-scale image first and blurred later, we are going to write a function that does this for us:\nfunc convertFrame(src gocv.Mat, dst *gocv.Mat) { gocv.Resize(src, \u0026amp;src, image.Point{X: 500, Y: 500}, 0, 0, gocv.InterpolationLinear) gocv.CvtColor(src, dst, gocv.ColorBGRToGray) gocv.GaussianBlur(*dst, dst, image.Point{X: 21, Y: 21}, 0, 0, gocv.BorderReflect101) } Lets walk through that function. First, we are resizing the image with the Resize function since working with squared images is a lot easier and faster. Then we use the CvtColor function to convert the image to gray-scale and finally, we apply the GaussianBlur to the gray image so that we get the blurred image we wanted.\nNow rewrite your previous program to call this function each time we read a new frame(including the first one). Basically add the line convertFrame(img, \u0026amp;img) after each video.Read, where img is the gocv.Mat you used to read a new frame. If all went well, then the window should display images that look kinda like this: We have the first frame and the current frame already converted to gray scale and blurred, now we need to compute the absolute difference between those two and apply the required threshold and dilation to the resulting difference:\ngocv.AbsDiff(firstFrame, frame, \u0026amp;difference) gocv.Threshold(difference, \u0026amp;difference, 50, 255, gocv.ThresholdBinary) gocv.Dilate(difference, \u0026amp;difference, kernel) The name of this functions are a bit self explanatory. AbsDiff computes the absolute difference between the first frame and the new frame and stores that difference on the difference mat. Threshold converts the gray image to a binary image by applying a fixed-level thresholding to each pixel and finally, Dilate dilates the image by using the specific kernel element, in this case our kernel is empty.\nThe best thing you can do to understand all this better is to read the docs for each function and play around with the values that they receive, for example, what happens when you change those two magic numbers that the Threshold function receives? What about the type of the threshold?\nIf you are curious here, here are most of the different types of thresholds that you can apply to an image: Lets add those filters to our code and we\u0026rsquo;ll display the difference mat instead of the normal frame mat we\u0026rsquo;ve been displaying so far:\npackage main import ( \u0026#34;image\u0026#34; \u0026#34;gocv.io/x/gocv\u0026#34; ) var ( firstFrame = gocv.NewMat() frame = gocv.NewMat() difference = gocv.NewMat() kernel = gocv.NewMat() ) func main() { window := gocv.NewWindow(\u0026#34;Motion\u0026#34;) video, _ := gocv.OpenVideoCapture(0) video.Read(\u0026amp;firstFrame) convertFrame(firstFrame, \u0026amp;firstFrame) for { video.Read(\u0026amp;frame) convertFrame(frame, \u0026amp;frame) gocv.AbsDiff(firstFrame, frame, \u0026amp;difference) gocv.Threshold(difference, \u0026amp;difference, 50, 255, gocv.ThresholdBinary) gocv.Dilate(difference, \u0026amp;difference, kernel) window.IMShow(difference) if window.WaitKey(1) == 27 { break } } video.Close() window.Close() firstFrame.Close() frame.Close() } func convertFrame(src gocv.Mat, dst *gocv.Mat) { gocv.Resize(src, \u0026amp;src, image.Point{X: 500, Y: 500}, 0, 0, gocv.InterpolationLinear) gocv.CvtColor(src, dst, gocv.ColorBGRToGray) gocv.GaussianBlur(*dst, dst, image.Point{X: 21, Y: 21}, 0, 0, gocv.BorderReflect101) } Try running this program by first having the camera point directly at you when the program first starts. What happens when you move around? Or even better, what happens when you leave the chair? You are going to see yourself all bright, but you aren\u0026rsquo;t there, are you? What kind of sorcery is this!!??\nWell that\u0026rsquo;s what it\u0026rsquo;s supposed to happen. Remember that all the filters we are applying are over the absolute difference of the first frame and the current frame. This means that if you are on the first frame you will be part of the \u0026ldquo;background\u0026rdquo; and whenever you leave the image there will be a difference on that area since instead of you sitting there the image will pick up everything that was behind you. This is why you see yourself sitting there even though you are not. It\u0026rsquo;s either that or magic, who knows.\nTry running this code without you being there for the first frame. Once the program is running pop in front of the camera, you should be looking into an image that resembles this: OK, we have our big bright spot on our image, now is time that we find some contours. I have to admit I thought this was going to be really hard, but thanks to GoCV\u0026rsquo;s FindContours function all we have to do is call it, iterate over all the contours and pick the contour that has the biggest area. Lets write a function that does exactly that but it also makes sure that the areas are bigger than some minimum:\n// bestContour obtains the biggest contour in the frame provided is bigger // than the minArea. func bestContour(frame gocv.Mat, minArea float64) []image.Point { cnts := gocv.FindContours(frame, gocv.RetrievalExternal, gocv.ChainApproxSimple) var ( bestCnt []image.Point bestArea = minArea ) for _, cnt := range cnts { if area := gocv.ContourArea(cnt); area \u0026gt; bestArea { bestArea = area bestCnt = cnt } } return bestCnt } We will call this function with the difference mat, since that is the one where we applied all the filters. After this function ends, we will either have the biggest contour that was found on the image or nothing at all. We can see whether we have results or not by checking the len of the []image.Point array that was returned. If we have a contour then we will draw a rectangle on the base image to show were was the area of movement found:\npackage main import ( \u0026#34;image\u0026#34; \u0026#34;image/color\u0026#34; \u0026#34;gocv.io/x/gocv\u0026#34; ) var ( firstFrame = gocv.NewMat() frame = gocv.NewMat() gray = gocv.NewMat() difference = gocv.NewMat() kernel = gocv.NewMat() motion = gocv.NewWindow(\u0026#34;Motion\u0026#34;) threshold = gocv.NewWindow(\u0026#34;Threshold\u0026#34;) rectColor = color.RGBA{G: 255} textColor = color.RGBA{B: 255} statusPoint = image.Pt(10, 20) ) func main() { video, _ := gocv.OpenVideoCapture(0) motion.ResizeWindow(500, 500) threshold.ResizeWindow(500, 500) motion.MoveWindow(0, 0) threshold.MoveWindow(500, 0) video.Read(\u0026amp;firstFrame) convertFrame(firstFrame, \u0026amp;firstFrame) for { video.Read(\u0026amp;frame) convertFrame(frame, \u0026amp;gray) gocv.AbsDiff(firstFrame, gray, \u0026amp;difference) gocv.Threshold(difference, \u0026amp;difference, 50, 255, gocv.ThresholdBinary) gocv.Dilate(difference, \u0026amp;difference, kernel) cnt := bestContour(difference.Clone(), 5000) if len(cnt) == 0 { if imShow() { break } continue } rect := gocv.BoundingRect(cnt) gocv.Rectangle(\u0026amp;frame, rect, rectColor, 2) gocv.PutText(\u0026amp;frame, \u0026#34;Motion detected\u0026#34;, statusPoint, gocv.FontHersheyPlain, 1.2, textColor, 2) if imShow() { break } } video.Close() motion.Close() threshold.Close() firstFrame.Close() gray.Close() difference.Close() frame.Close() } func imShow() bool { motion.IMShow(frame) threshold.IMShow(difference) return motion.WaitKey(1) == 27 || threshold.WaitKey(1) == 27 } func convertFrame(src gocv.Mat, dst *gocv.Mat) { gocv.Resize(src, \u0026amp;src, image.Point{X: 500, Y: 500}, 0, 0, gocv.InterpolationLinear) gocv.CvtColor(src, dst, gocv.ColorBGRToGray) gocv.GaussianBlur(*dst, dst, image.Point{X: 21, Y: 21}, 0, 0, gocv.BorderReflect101) } // bestContour obtains the biggest contour in the frame provided is bigger // than the minArea. func bestContour(frame gocv.Mat, minArea float64) []image.Point { cnts := gocv.FindContours(frame, gocv.RetrievalExternal, gocv.ChainApproxSimple) var ( bestCnt []image.Point bestArea = minArea ) for _, cnt := range cnts { if area := gocv.ContourArea(cnt); area \u0026gt; bestArea { bestArea = area bestCnt = cnt } } return bestCnt } We made a lot of changes in that code, lets walk through each of them. First off, we now have two windows instead of one, why? Well, we are going to be displaying two different types of images. In the motion window we will display the normal image with the rectangle drawn on top of the area of motion(if there is any). In the threshold window we will show the difference mat we\u0026rsquo;ve been showing so far. We also did a few resizes and moved the windows over so that they are displayed side by side.\nSince now we want to preserve the colors of the images we are reading, we can not use the same frame mat when we call convertFrame. This is why we have the new gray mat that we use for the conversion.\nThe most important change of the previous code is right here:\ncnt := bestContour(difference.Clone(), 5000) if len(cnt) == 0 { if imShow() { break } continue } rect := gocv.BoundingRect(cnt) gocv.Rectangle(\u0026amp;frame, rect, rectColor, 2) gocv.PutText(\u0026amp;frame, \u0026#34;Motion detected\u0026#34;, statusPoint, gocv.FontHersheyPlain, 1.2, textColor, 2) First, we call the bestContour function to obtain the biggest area of movement. If there are no results, we simply call the new imShow image that displays the images without drawing anything on them. But if we have results then we first have to find the rectangle of that area using the BoundingRect function, once we have that we simply draw the rectangle on our image with Rectangle and display the \u0026ldquo;Motion detected\u0026rdquo; text on our image.\nIf all went well, when you run this code and pop in front of the camera you should have something similar to this: And voila! You have motion detection! Don\u0026rsquo;t go out and celebrate with a beer yet. We need to compile this to a binary that can run on the RPi, move that binary there and run it. In order to see this program running on the raspberry pi, you will need to have a display connected to it.\nYou can compile the binary for the RPi with the following command:\nGOOS=linux GOARCH=arm GOARM=5 go build Once you have your binary, you can send it over with scp:\nscp \u0026lt;USER\u0026gt;@\u0026lt;RASPBERRY IP\u0026gt;:\u0026lt;DIRECTORY ON THE PI\u0026gt; \u0026lt;BINARY-NAME\u0026gt; Change those parameters accordingly and run the command. SSH into the RPi and run the binary, if you have a video camera connected to the Pi that can be identified with the same ID you\u0026rsquo;ve been using, then you should see the same image you saw when you ran this program on your development machine.\nCongrats! Now you can go and have some beers!\nConclusion On this blog post you saw how to build a simple and lightweight motion detection program with GoCV and how to compile and run that on the raspberry pi. The idea is to connect two servos and make them follow the area of movement. All that will be explained in Part 2 and Part 3 of this blog post, so stick around for that!\nThank you!\n","permalink":"https://blog.matiaspan.dev/posts/motion-tracking-turret-with-gobot-and-gocv/","summary":"The title says it, motion tracking turret using only Go, ready to have some func?\nThis blog post will be divided into three main parts: Motion detection with GoCV, Controlling servo motors with Gobot and Putting it all together. In the first part, we build a lightweight motion detection algorithm that can run on low-power devices such as the RPi. In the second part we will show how to control servo motors with Gobot from the Raspberry Pi and in the last part we\u0026rsquo;ll explain how to go from detecting an object on an image to telling exactly the angles in which the servos need to move in order to track it.","title":"Motion tracking turret with Gobot and GoCV - Part 1"},{"content":"In this post I will show step by step how to create a Kubernetes cluster on DigitalOcean and then deploy Gogs to the cluster using a set of tools that automate all this.\nCreating the Cluster Choosing the Cloud Provider I chose DigitalOcean as the cloud provider since it\u0026rsquo;s the cheapest I could find and it has worked really well for me in the past, plus I was already familiar with a few of their products. You could follow this tutorial using a different provider but there will be a few differences when configuring the persistent volumes for gogs.\nGo ahead and create an account there if you don\u0026rsquo;t already have one. Note that they will request you to register a valid credit or debit card but they won\u0026rsquo;t charge you anything as long as you destroy the cluster once you are done with this post but if you leave it running it won\u0026rsquo;t cost more than 30 USD per month.\nStackPointCloud StackPointCloud is a web application that allows us to create a Kubernetes cluster on many different cloud provider with no more than a few clicks. The cluster they deploy also comes with a few nice things already configured such as Helm and the Kubernetes Dashboard.\nIf you really want to learn what it\u0026rsquo;s like to setup a cluster on your own you could use Ansible to automate the installation of kubernetes on the machines and then configure helm, the dashboard and coreDNS but it will take much more than just a few clicks. If you feel like it you can follow this tutorial, it took a while for me to get it working since I was not familiar with any of the tools mentioned there but it was really fun.\nAfter you created an account on StackPointCloud click on the Create cluster link and select DigitalOcean as provider. After that a window looking like this will be opened:\nOn the left you see all the information of cluster that it\u0026rsquo;s being created. By default StackPoint recommends 1 master and 2 workers, all using coreOS with 2GB of RAM and 50GB of storage in the NYC3 region. If you want you could edit the cluster details by clicking edit and modifying the fields there:\nI\u0026rsquo;m going to leave it as is since it is a good and simple configuration for this use case.\nYou need to create a DigitalOcean API token and provide it to StackPoint so that they can provision the droplets and configure everything. Head over to DigitalOcean\u0026rsquo;s cloud console and create a new personal access token by clicking on the Generate New Token link:\nOnce you generated your token copy it over to the StackPoint configuration. Next you will generate an SSH key(or use an existing one) on your machine and add it to StackPoint.\nNow you are ready to hit create and wait since it takes a while for all droplets to be provisioned and configured. Once you are done you should see the info of the cluster and it should look somewhat like this:\nThats it!! You just created a 3 node kubernetes cluster, it really was no more than a few clicks!\nWe are almost done with the cluster setup. Now you need to configure your local kubectl installation to point to the recently created cluster(if have not installed kubectl yet follow this tutorial). In the StackPoint cluster page previously showed you\u0026rsquo;ll see a link on the left that says kubeconf, download that file and copy its contents to ~/.kube/config. To check if it was properly configured run kubectl cluster-info and the output should look similar to this:\nKubernetes master is running at https://\u0026lt;MASTER IP\u0026gt;:\u0026lt;MASTER PORT\u0026gt; Heapster is running at https://\u0026lt;MASTER IP\u0026gt;:\u0026lt;MASTER PORT\u0026gt;/api/v1/namespaces/kube-system/services/heapster/proxy KubeDNS is running at https://\u0026lt;MASTER IP\u0026gt;:\u0026lt;MASTER PORT\u0026gt;/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Installing Gogs Gogs is a lightweight git server that can deployed very easily since it does not have many dependencies and features like Gitlab CE.\nTo install Gogs on our cluster we are going to be using Helm so if you don\u0026rsquo;t already have it installed and configured follow this tutorial. After that you need to add the incubator repo since that is where the gogs chart lives:\nhelm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com/ Since we are going to change a lot of the default values that come with the gogs chart you need to clone the charts repository:\ngit clone https://github.com/helm/charts.git Inside the charts repo you will have the folder incubator that holds the gogs chart. In there you\u0026rsquo;ll find the values.yml file that is used when installing gogs, this file would be good for local installations but since we are installing this on a cloud provider we are going to have to change a few things.\nThis is the resulting values.yml, below I\u0026rsquo;ll explain what I have changed and which properties you need to configure:\nserviceType: LoadBalancer replicaCount: 2 image: repository: gogs/gogs tag: 0.11.29 pullPolicy: IfNotPresent service: httpPort: 80 sshPort: 22 sshDomain: CHANGEME gogs: appName: \u0026#39;Some name\u0026#39; runMode: \u0026#39;prod\u0026#39; forcePrivate: true disableHttpGit: false repositoryUploadEnabled: true repositoryUploadAllowedTypes: repositoryUploadMaxFileSize: 50 repositoryUploadMaxFiles: 5 serviceEnableCaptcha: false serviceRegisterEmailConfirm: false serviceDisableRegistration: false serviceRequireSignInView: true serviceEnableNotifyMail: false cacheAdapter: memory cacheInterval: 60 cacheHost: serverDomain: CHANGEME serverRootUrl: http://CHANGEME serverLandingPage: home databaseType: postgres databaseHost: databaseUser: databasePassword: databaseName: webhookQueueLength: 1000 webhookDeliverTimeout: 5 webhookSkipTlsVerify: true webhookPagingNum: 10 logMode: console logLevel: Trace otherShowFooterBranding: false otherShowFooterVersion: true otherShowFooterTemplateLoadTime: true securitySecretKey: \u0026#34;changeme\u0026#34; uiExplorePagingNum: 20 uiIssuePagingNum: 10 uiFeedMaxCommitNum: 5 ingress: enabled: false persistence: enabled: true storageClass: \u0026#34;do-block-storage\u0026#34; accessMode: ReadWriteOnce size: 20Gi postgresql: install: true postgresUser: gogs postgresPassword: gogs postgresDatabase: gogs persistence: enabled: true storageClass: \u0026#34;do-block-storage\u0026#34; size: 5Gi Ok, that\u0026rsquo;s a big file and lots of configs. I\u0026rsquo;m not going to go over each value and explain what it means(I don\u0026rsquo;t even know), but lucky for you the default values.yml has a lot of comments that explain quite well each config.\nThis are the fields that were changed and the ones you need to setup:\nserviceType: previously it was setup as NodePort which is fine for local setups but since we are running in the cloud we need to specify it as a LoadBalancer. replicaCount: it was changed from 1 to 2 so that I always have two instances running. sshDomain: this will be the domain used when gogs renders the SSH urls for clones, if you plan to setup a domain name on top of this you can use that, if not then set whatever you\u0026rsquo;d like since this doesn\u0026rsquo;t affect the functionality of gogs it\u0026rsquo;s just that the UI will be lying to you saying to clone it as git clone git@localhost:user/repo.git or something like that. forcePrivate: set to true so that by default the UI checks the private checkmark when creating a new repository. repositoryUploadMaxFileSize: set to 50(in MB) so that the maximum file size that can be uploaded is of 50MB. serverDomain: the domain of gogs, same as with SSH. Use your domain name. serverRotUrl: basicaly http or https plus the serverDomain. storageClass: this is under the persistence section, you need to specify the storageClass used for persistent volumes, if you were using DigitalOcean then leave it with do-block-storage. Otherwise check your storage class with kubectl get storageClass and use that value. storageClass: this is the one found under the postgresql section and you should put the same value as before. Now that all the configurations are in place we can use helm to install gogs on our cluster. Inside the charts repo you recently cloned execute the following command:\nhelm install --name gogs -f incubator/gogs/values.yaml incubator/gogs This magic command will install and configure all the pods, services, deployments and load balancers needed to run 2 replicas of gogs.\nIt will take a while before gogs is able to be used, you can check the status of the installation with the following command:\nhelm status gogs In the output of the helm install command you will find a few commands that will also be useful for checking the status of the installation, this one will allow you to know when the IP of the LoadBalancer is ready:\nkubectl get svc --namespace {{ .Release.Namespace }} -w {{ template \u0026#34;gogs.fullname\u0026#34; . }} After it is ready execute the following three commands to find out where exactly is gogs living:\nexport NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services {{ template \u0026#34;gogs.fullname\u0026#34; . }}) export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT/ The output of the last command will show you the URL for accessing gogs. Head over to that URL and register a user, this user will be the admin of the application.\nConclusion Congrats!!! You just created your own scalable git server running on a kubernetes cluster, it\u0026rsquo;s amazing how the tools we have available have reduced the complexity of things such as automated deployment, scalability and availability.\n","permalink":"https://blog.matiaspan.dev/posts/deploying-gogs-to-a-custom-kubernetes-cluster/","summary":"In this post I will show step by step how to create a Kubernetes cluster on DigitalOcean and then deploy Gogs to the cluster using a set of tools that automate all this.\nCreating the Cluster Choosing the Cloud Provider I chose DigitalOcean as the cloud provider since it\u0026rsquo;s the cheapest I could find and it has worked really well for me in the past, plus I was already familiar with a few of their products.","title":"Deploying Gogs to a DigitalOcean Kubernetes cluster"}]